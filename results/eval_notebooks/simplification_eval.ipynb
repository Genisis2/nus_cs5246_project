{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of ACCESS and Rule-Based Simplification on the BART Summary Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Straits Times summaries generated by BART."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_st_sum = pd.read_csv(\"results/system_outputs/bart_summary.csv\",encoding = \"ISO-8859-1\")\n",
    "st_sum_dict =  dict({'article': [] ,'summary_text': []})\n",
    "\n",
    "\n",
    "for i in df_st_sum['article']:\n",
    "    st_sum_dict['article'].append(i)\n",
    "for i in df_st_sum['summary_text']:\n",
    "    st_sum_dict['summary_text'].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:\n",
      "    Article: SINGAPORE  The construction of the Johor Bahru-Singapore Rapid Transit System (RTS) Link is progressing well, with 45 per cent of the work on the Singapore side completed. Transport Minister S. Iswaran provided the update on Friday when he visited the work site in Admiralty Road West. He said: We are on track to achieve the completion goal so that the systemcan be operational by the end of 2026, and this is what both sides are working towards. Mr Iswaran described the new link as an important addition to the existing road connectivities between Singapore and Malaysia as it will also promote people to people, economic and other linkages as well.  When the 4km RTS Link shuttle service starts operating, passengers will be able to travel from the Bukit Chagar station in Johor Bahru to the Woodlands North station, or in the reverse direction, in about five minutes.  The train service can serve up to 10,000 passengers per hour in each direction.  Passengers will also be able to transfer from the RTS station to the Thomson-East Coast Line concourse via an underground link, without needing to exit the station. The RTS is expected to help ease congestion on the Causeway.  In March 2023, the Immigration and Checkpoints Authority (ICA) in Singapore said that peak-hour traffic volumes through the Woodlands and Tuas checkpoints have returned to pre-Covid-19 levels. On Friday, Mr Iswaran took a boat ride out to the Strait of Johor to view one of the foundations on the Singapore side of the project  the furthest from the island.  Done in three stages, 5,000 cubic m of concrete  enough to fill 2 Olympic-size swimming pools  are needed to build just one of the 12 foundations to be used for the 730m-long rail viaduct on the Singapore side of the RTS Link. Nine of the 12 foundations are located in the Strait of Johor. On average, they reach 30m deep into the seabed. The remaining three are sited on land.  Mr Iswaran said that construction of the 12 foundations is expected to be completed by the first quarter of 2024.  When the platforms are done, vertical columns will be erected to shoulder concrete segments that form the rail viaduct.  A total of 207 such segments will be built. Each weighs as much as 180 tonnes,and spans between 50m and 110m in length.  The massive weight of these segments poses an engineering challenge to lift them into position safely. In the middle of the Strait of Johor, the viaduct will stand 25m above the water level. As it nears the Singapore side, the height will be lowered gradually to meet the underground tunnels connecting to the underground RTS Link Woodlands North station. Commuters will be able to connect to the new Customs, immigration and quarantine (CIQ) building through an underground link which is being built.  As will be the case at Bukit Chagar station, the CIQ facilities of both Singapore and Malaysia will be co-located within the same building at Woodlands North.  This means that passengers need to only clear the immigration authorities at their point of departure instead of having to do it a second time when they arrive, as is the current practice for land border crossings. In a statement, The Land Transport Authority called the RTS Link a game changer that will significantly improve connectivity between Singapore and Johor Bahru, and ease congestion along the Causeway.\n",
      "    BART Sum.: The Johor Bahru-Singapore Rapid Transit System (RTS) Link is progressing well. 45 per cent of the work on the Singapore side has been completed. Transport Minister S. Iswaran provided the update on Friday when he visited the work site. He said: We are on track to achieve the completion goal.\n",
      "2:\n",
      "    Article: NEW YORK - Indicted FTX cryptocurrency exchange founder Sam Bankman-Fried pleaded not guilty on Thursday to new US charges of conspiring to violate campaign finance laws and bribe Chinese authorities. Bankman-Fried, 31, entered the plea to the new, 13-count indictment through his lawyer, Mr Mark Cohen, at a hearing before United States District Judge Lewis Kaplan in a Manhattan federal court. He had earlier pleaded not guilty to eight counts of fraud and conspiracy for allegedly stealing billions in FTX customer funds to plug losses at his hedge fund, Alameda Research. Mr Cohen said he was planning to challenge the new charges because they were brought after Bankman-Fried was extradited from the Bahamas, where he was arrested last December and where FTX was based. The new charges add to the pressure on Bankman-Fried, who faces a possible sentence of decades in prison if convicted at a trial set to start on Oct 2. He was arrested after a flurry of customer withdrawals spurred by concerns about commingling of funds between the exchange and Alameda prompted the collapse of the now-bankrupt FTX. The initial indictment by the US Attorneys office in Manhattan contained few details about the alleged scheme. In an unusual post-arrest blog post, the former billionaire acknowledged inadequate risk management at FTX, but said he did not steal funds. In late February, prosecutors filed a new 12-count indictment elaborating on the fraud charges and accusing Bankman-Fried of illicitly contributing tens of millions of dollars to US political campaigns through straw donors, part of a strategy to buy influence in Washington. And on Tuesday, prosecutors moved to unseal yet another indictment, which accused Bankman-Fried of conspiring to violate an anti-bribery law by orchestrating a US$40 million (S$53 million) payment to Chinese authorities to regain access to US$1 billion in cryptocurrency in Alameda accounts that had been frozen. Three former members of Bankman-Frieds inner circle  former Alameda chief executive Caroline Ellison, former FTX technology chief Zixao Gary Wang and former FTX engineering director Nishad Singh  have all pleaded guilty and agreed to cooperate with prosecutors. Bankman-Fried is confined to his parents Palo Alto, California, home on a US$250 million bond pending trial. Earlier this week, Judge Kaplan approved modifications to Bankman-Frieds bail package that are designed to prevent the defendant from tampering with witnesses. \n",
      "    BART Sum.: Bankman-Fried, 31, entered the plea to the new, 13-count indictment through his lawyer. He had earlier pleaded not guilty to eight counts of fraud and conspiracy. He faces a possible sentence of decades in prison if convicted at a trial set to start on Oct 2.\n",
      "3:\n",
      "    Article: SHANGHAI  China will encourage foreign capital to participate in its financial markets and may allow foreign-funded financial institutions to go public in the country when conditions are ripe, local media quoted a former finance minister as saying on Saturday. Such moves would be in line with the countrys opening up of its financial industry, Mr Lou Jiwei told the Global Asset Management Forum in Beijing, according to the 21st Century Business Herald newspaper. At the same event, Mr Cao Yu, vice-chairman of the China Banking and Insurance Regulatory Commission, was quoted by Shanghai Securities News as saying China would respond to the demands of foreign financial institutions and promote the common development of Chinese and foreign-funded banking and insurance entities. Beijing has been stepping up efforts to woo foreign companies and investors to aid an economic recovery after the dismantling of its zero-Covid policy late last year.  It has sped up fund licence approvals for foreign asset managers in recent months. Mr Lou also commented on the recent collapse of Silicon Valley Bank in the United States, saying Chinese authorities attach great importance to preventing and resolving systemic risks and are trying to improve financial supervision with the creation of a financial regulatory body. We will also continue to cooperate with the financial regulatory agencies of other countries to jointly prevent and resolve systemic risks in the global financial system and maintain the stability and prosperity of the global financial market, he added, according to the newspaper. \n",
      "    BART Sum.: Beijing has been stepping up efforts to woo foreign companies and investors. It has sped up fund licence approvals for foreign asset managers in recent months. Former finance minister: China will encourage foreign capital to participate in its financial markets and may allow foreign-funded financial institutions to go public in the country.\n",
      "4:\n",
      "    Article: TAOYUAN, Taiwan  Tension with China has escalated under Taiwans government and the island will in future have to choose between peace and war, former Taiwanese president Ma Ying-jeou said on Friday at the end of a landmark visit to China. Mr Ma is the first former Taiwanese president to visit China. Since the defeated Republic of China government fled to Taiwan in 1949 after losing a civil war to Mao Zedongs communists, no serving island leader has visited China. Our administration continues to lead Taiwan to danger. The future is a choice between peace and war, Mr Ma told reporters at Taiwans main airport after arriving from Shanghai at the end of his 12-day visit to China. Mr Ma was president from 2008 to 2016 as the head of a Kuomintang (KMT) government. The party, now in opposition, favours close ties with China, which claims the island as its own. His visit came at a time of heightened tension, with Chinas anger roused this week by a meeting between Taiwanese President Tsai Ing-wen and US House Speaker Kevin McCarthy during a stopover by the Taiwan leader in the United States. Beijing has been stepping up its political and military pressure to get democratically governed Taiwan to accept Chinese sovereignty. Ms Tsai and her government reject that and say only the islands people can decide their future. The ruling Democratic Progressive Party (DPP) criticised Mr Mas trip, but he said it had proven that Taiwan and China could engage under the principle that both are part of a single China, although each can have its own interpretation of the term. Mr Ma said Taiwan could share a common political basis with China, which would be in the best interests of the people of Taiwan. Ms Tsais DPP said in a statement that Mr Ma had become an accomplice of Beijings one China principle and he had failed to take the opportunity to defend Taiwans sovereignty. Ms Tsai has offered talks with China, but Beijing, which views her as a separatist, has rebuffed her. Mr Ma met Chinese President Xi Jinping in 2015 in Singapore, shortly before Ms Tsai was elected president, but he did not meet the Chinese leader on this trip. He visited historic sites in several cities including Wuhan, where he met Mr Song Tao, the head of Chinas Taiwan Affairs Office. The KMT has defended its contacts with China, saying it is trying to reduce tension and that it will trumpet that line in the run-up to a presidential election in January. Mr Ma said he would continue to work in a private capacity to ensure Taiwan has a future of real peace and safety. \n",
      "    BART Sum.: Taiwan will have to choose between peace and war, former president Ma Ying-jeou says. Mr Ma is the first former Taiwanese president to visit China. He was president from 2008 to 2016 as the head of a Kuomintang government. The party favours close ties with China, which claims the island as its own.\n",
      "5:\n",
      "    Article: ULSAN  South Korea squandered a two-goal lead in Jurgen Klinsmanns first game in charge on Friday, and had to settle for a 2-2 draw against Colombia. A brace for captain Son Heung-min gave the hosts a 2-0 lead at half-time at the Munsu Football Stadium in Ulsan. But two Colombia goals within five minutes of the restart restored parity, with neither side finding a winner. Son opened the scoring 10 minutes in, with help from a Colombian gaffe. Goalkeeper Camilo Vargas inexplicably vacated his net after passing the ball to a teammate, leaving Son  who won possession high up the field  to easily stick the ball into the empty goal with his left foot. The Spurs star picked up his second near the end of the first half with a free kick, this time using his right foot to beat Vargas at the bottom-left corner. Colombia, who had barely had a sniff in the first half, turned the tables in the second with two quick goals. James Rodriguez halved the deficit in the 46th minute, sneaking in behind multiple defenders to convert a pass from Diego Valoyes. Jorge Carrascal netted the equaliser three minutes later, set up by Daniel Munoz following a South Korean turnover. Colombia almost took the lead in the 69th minute, with Rafael Santos Borres header hitting the crossbar. With only a few minutes left, South Korea also came close to scoring as substitute Oh Hyeon-gyu rolled a left-footed shot past the diving Vargas  only for a defender to clear the ball out of harms way. \n",
      "    BART Sum.: Son Heung-min scored twice as South Korea were held to a 2-2 draw by Colombia. James Rodriguez and Jorge Carrascal scored in the second half to level the scores. Jurgen Klinsmanns first game in charge of South Korea was in Ulsan.\n",
      "6:\n",
      "    Article: JOHANNESBURG  South African Paralympic champion Oscar Pistorius may be released from prison this week, a decade after he killed his girlfriend in a crime that gripped the world.  A parole board is to decide whether the 36-year-old should be let out early, after a hearing in Pretoria on Friday.  The board must determine whether the purpose of imprisonment has been served, said Department of Correctional Services spokesman Singabakho Nxumalo. Pistorius shot dead Reeva Steenkamp, a model, on Valentines Day in 2013, when he fired four times through the bathroom door of his Pretoria house. He pleaded not guilty and denied that he killed Steenkamp in a rage, saying he mistook her for a burglar. Known worldwide as the Blade Runner because of his carbon-fibre prosthetics, he was sentenced to 13 years behind bars. Offenders in South Africa are automatically eligible for parole consideration after serving half of their sentence. Pistorius has served more than half, having started his term in 2014. As part of his rehabilitation, he met Steenkamps parents June and Barry in 2022 in a process authorities said aims to ensure inmates acknowledge the harm they have caused to their victims and the society at large. June will attend the hearing to make representations to the parole board which will include both parents victim impact statements, said Tania Koen, a lawyer representing the Steenkamps. Barry is unable to travel due to ill health, Koen added.  They are going to make their submissions, before the law will take its course, she said. The board, normally made up of correctional services and community members, will consider whether an inmate has been rehabilitated or still poses a danger to society, said Kelly Phelps, a law professor at the University of Cape Town. This will take into account the seriousness of the offence as well as Pistorius behaviour behind bars, including whether he attended educational and life skills programmes.   Release on parole usually comes with some conditions, such as monitoring from authorities and duty to report to a community correction centre. Day parole, where the inmate returns to prison at night, and community service are also on the cards. A year before killing Steenkamp, Pistorius became the first double amputee to race at the Olympics at the London 2012 games. He then was a sporting icon admired worldwide and courted by sponsors. But his achievements came crashing down after the killing. He was initially sentenced to six years in jail, but the term was later lengthened to 13 after the state appealed that it was unduly lenient. \n",
      "    BART Sum.: Paralympic champion Oscar Pistorius may be released from prison this week. A parole board is to decide whether the 36-year-old should be let out early. Pistorius shot dead Reeva Steenkamp on Valentines Day in 2013. Known worldwide as the Blade Runner because of his carbon-fibre prosthetics.\n",
      "7:\n",
      "    Article: SINGAPORE  An appeal by the man accused of killing art student Felicia Teo in 2007 to be acquitted of a murder charge was on Thursday dismissed by Chief Justice Sundaresh Menon. Ahmad Danial Mohamed Rafaee, 38, had sought a discharge amounting to an acquittal, which would have completely cleared him of the charge without a trial. Ms Teo went missing in 2007, when she was 19. Ahmad was among the few people who had been with her when she was last seen alive. A partial skull, which was later determined as likely to be of Ms Teo, was found in June 2010 during excavation works in Punggol. No other remains were found. Ahmad was charged with murder in December 2020, but was given a discharge not amounting to an acquittal by a district judge in July 2022. It means Ahmad can still be prosecuted if relevant information or evidence is to emerge. The Chief Justice, in upholding the lower courts decision, noted that Mr Ragil Putra Setia Sukmarahjana, who was also allegedly involved in Ms Teos death, is still a live lead. The Chief Justice said as things stand, the murder charge against Ahmad cannot proceed until and unless Mr Ragils assistance is secured. Prosecutors said they have reason to believe Mr Ragil is in Indonesia, and that they are in contact with the Indonesian authorities to secure his further assistance. The Chief Justice said a discharge not amounting to an acquittal struck the correct balance between the public interest in completing investigations into Ms Teos death and Ahmads personal interest in being freed from the hardships that come from having an unresolved murder charge hanging over him. A discharge not amounting to an acquittal allows the state to revive proceedings, while the accused is free of some restrictions that come with having been charged, such as being remanded in custody, the Chief Justice said. The more serious the charge, the more the balance would tilt in favour of public interest, he noted.  In the present case, the public interest in enabling the investigations to be completed is perhaps at the highest end of the scale, he added.  Ahmads lawyer, Mr Shashi Nathan, pointed to the fact that the offence occurred some 16 years ago. But the Chief Justice noted that a delay in the conduct of investigations typically carries less weight when the delay is caused by the accused person. Police had questioned Ahmad after Ms Teo went missing in 2007, but he claimed then that he did not know what had happened to her. In 2020, a review of the case led to Ahmad being questioned again. This time, he revealed that he was involved in the disposal of Ms Teos remains, that he was involved in the disposal of her possessions, and that he had not been truthful in 2007. Last October, Ahmad was sentenced to 26 months jail on four charges relating to Ms Teos death  including one for dumping her corpse and one for giving false evidence to the police. The sentence was backdated to when he was arrested on Dec 15, 2020. On the murder charge, the prosecution sought a discharge not amounting to an acquittal as Mr Ragil was still at large and efforts to trace him in Indonesia were ongoing.  On Thursday, Mr Nathan argued that it was unfair for his client to have the murder charge hanging over his head indefinitely and questioned the efforts made to locate Mr Ragil. Mr Nathan also cited, as an example of the hardship caused to Ahmad, the fact that his application for his passport to be renewed was rejected because of the case. But Deputy Public Prosecutor Yang Ziliang argued that the seriousness of the offence involved weighed heavily against an acquittal at this juncture.  An acquittal would grant Ahmad absolute immunity from ever being taken to task for the murder of the victim even if evidence of such involvement subsequently came to light, said the prosecutor.\n",
      "    BART Sum.: Ahmad Danial Mohamed Rafaee, 38, had sought a discharge amounting to an acquittal. This would have completely cleared him of the charge without a trial. Ms Teo went missing in 2007, when she was 19. Ahmad was among the few people who had been with her when she went missing.\n",
      "8:\n",
      "    Article: SINGAPORE  Construction of the Jurong Region Line (JRL) moved up a gear on Friday after more than three years of preparation works, with the official ground-breaking and launch of the track viaduct in the Tengah area.  Slated to open in three stages from 2027 to 2029, Singapores seventh MRT line is projected to have its daily ridership grow from 200,000 in the initial years to more than 500,000 along with the development of the Jurong Innovation District, Tengah Town and Jurong Lake District, said the Land Transport Authority (LTA).  The 24km-long MRT line has 24 stations, with three interchanges  Boon Lay, Choa Chu Kang and Jurong East  to connect commuters to other MRT lines.  The JRL is expected to significantly improve connectivity in the western part of Singapore and support developments in the Jurong area, said the LTA, adding that more than 60,000 additional households are projected to be within a 10-minute walk to a train station when the line is completed.  Commuters travelling to the western part of Singapore will enjoy shorter journeytimes, said the LTA, citing how the JRL will shave 25 minutes off the current one-hour journey from Choa Chu Kang MRT station to Nanyang Technological Universitys Lee Wee Nam Library via bus and train.  On Friday, contractors hoisted up one of the concrete segments of the track viaduct as part of the launch ceremony. These concrete segments can weigh up to 40 tonnes each. In all, there will be approximately 12,000 such segments to form the 24km line, which will snake around or cut across existing infrastructure such as busy roads, major expressways and canals.  In a speech at the event, Transport Minister S. Iswaran said building the line through such a densely developed corridor poses stiff challenges.  At the three interchange stations  Boon Lay, Choa Chu Kang and Jurong East  some of the existing station structures have to be taken down and strengthened, he noted.  To minimise disruptions at these busy stations, construction activities have been carefully staged and virtual reality technologies used to optimise station layouts and ensure commuters can still move smoothly from point to point, he said. Some parts of the viaduct will be close to existing buildings, like Housing Board blocks  within 10m to 15m, Mr Iswaran said. When asked, the LTA said one location where the track comes that close to existing buildings is the stretch between Choa Chu Kang Way and Choa Chu Kang Avenue 3. Permanent sound barriers will be installed along viaducts near homes to minimise noise disturbance, said LTA. When construction involves busy roads, the work will be done only late at night to minimise inconveniences. The LTA said traffic diversions along Choa Chu Kang Avenue 3, Jurong East Central and Jurong West Avenue 4 have already been implemented to allow for the construction of stations and the viaduct. The JRL will have 62 fully automated, driverless three-car trains, which will arrive from South Koreas Hyundai Rotem Company by the middle of 2024. The lines capacity can be expanded to four cars as demand increases. Mr Iswaran said the JRL trains will have slightly smaller carriages to negotiate tight curves along sections of the line.  For example, each JRL train carriage will be 5m shorter in length and 0.45m narrower in width compared with Circle Line train carriages. The LTA said JRL stations will be equipped with photovoltaic solar panels to provide energy for their operations, to reduce the carbon footprint from rail operations.  The design of the stations allows crossflow natural ventilation, said LTA, highlighting features such trickle-vents in the roof to allow hot air to escape and louvres that allow more air to flow through the platform help to reduce the reliance on fans and air-conditioning.  To reduce energy consumption, trains will have sensors that dim the cabin lights during the day. Similar to existing MRT lines, the JRL will have a regenerative braking system which uses energy produced by trains during braking to power nearby trains or stations, Mr Iswaran said.  When the JRL was announced in 2018, it was supposed to be opened in three stages from 2026 to 2028. This has since been pushed back by a year due to delays caused by the Covid-19 pandemic.  #PSA  Construction works for our 7th #MRT line  the #Jurong Region Line (#JRL)  have officially started!  \n",
      " ...\n",
      "    BART Sum.: Construction of the Jurong Region Line (JRL) moved up a gear on Friday. The official ground-breaking and launch of the track viaduct in the Tengah area was held. The 24km-long MRT line has 24 stations, with three interchanges. The JRL is expected to significantly improve connectivity in the western part of Singapore.\n",
      "9:\n",
      "    Article: KYIV - Ukraines top military brass on Thursday withdrew a report that wrongly said Russian troops had left the town of Nova Kahkovka in southern Kherson region, and blamed an error for the mistake. The general staff of the armed forces, in a rare retraction, said Russian troops remained in the town on the east bank of the Dnipro River. It said the initial report had been issued as a result of incorrect use of available data but gave no details. Mr Vladimir Saldo, the Russian-installed governor of Kherson region, had earlier denied the report. Russian forces redeployed to the east bank of the Dnipro River last November after abandoning positions on the west bank in the face of a counter-offensive by Ukrainian troops. \n",
      "    BART Sum.: Report wrongly said Russian troops had left town of Nova Kahkovka in southern Kherson region. General staff of the armed forces, in a rare retraction, said Russians remained in the town on the east bank of the Dnipro River. It said the initial report had been issued as a result of incorrect use of available data but gave no details.\n",
      "10:\n",
      "    Article: LISBON - Arsenal and Sporting Lisbon could not be divided in an entertaining 2-2 draw in their Europa League last 16 first leg clash in Lisbon on Thursday. Hidemasa Moritas own goal sent the Premier League leaders back to London with honours even, after Goncalo Inacio and Paulinhos goals helped Sporting recover from William Salibas opener. Elsewhere, Jose Mourinhos Roma earned a comfortable 2-0 win at home against Real Sociedad to put one foot into the quarter-finals. Mikel Arteta handed Reiss Nelson a start on the left of the attack after his showstopping strike salvaged Arsenal a dramatic comeback victory over Bournemouth last weekend to keep them top of the Premier League. The Arsenal coach also gave Polish defender Jakub Kiwior his debut after his January arrival from Spezia. It was his centre-back partner William Saliba who headed home from Fabio Vieiras corner to put Arsenal ahead after 22 minutes at the Jose Alvalade Stadium in Lisbon. Former Liverpool defender Sebastian Coates was booked during an argument in the aftermath of the goal, meaning he is suspended for the second leg next week. Sporting equalised shortly after in near identical fashion, with Inacio heading home Marcus Edwards corner. Former Tottenham midfielder Edwards then forced a fine save from Arsenal goalkeeper Matt Turner, as the hosts stepped up a gear. Sporting goalkeeper Antonio Adan did well to deny Gabriel Martinelli early in the second half, and his team capitalised to take the lead. Edwards clever reverse pass split open Arsenals defence and after Pedro Goncalves shot was deflected, Paulinho reacted quickest to finish. Martinelli nearly equalised with a brilliant run from halfway but, after rounding Adan, was foiled by a last-gasp Jeremiah St. Juste slide tackle. Arsenal were level just after the hour mark, though, when Granit Xhaka tried to find Martinelli but Morita deflected the ball into his own net to leave the game on a knife-edge in the second leg. Mourinhos Roma were good value for their victory against Real Sociedad with Stephan El Shaarawy sending the hosts ahead in the Italian capital from close range after a fine run and cross by Tammy Abraham. Take Kubo struck the post at the other end as the Basques battled back in search of an equaliser. Roma struck again towards the end as Albanian defender Marash Kumbulla powered home a header from Paulo Dybalas inswinging corner, giving them a strong advantage to take to San Sebastian. Bundesliga surprise package Union Berlin snatched a 3-3 draw against Belgian side Royale Union SG, while Bayer Leverkusen beat Ferencvaros 2-0 at home. In the Uefa Conference League, Michael Antonio hit two first half goals to earn West Ham a 2-0 win at EK Larnaca, while Villarreal shared a 1-1 draw with Anderlect. Manu Trigueros put the Spanish visitors in front but Anders Dreyer levelled early in the second half.     \n",
      "    BART Sum.: Arsenal and Sporting Lisbon draw 2-2 in their Europa League last 16 first leg clash in Lisbon. Hidemasa Moritas own goal sends Premier League leaders back to London with honours even. Goncalo Inacio and Paulinhos goals help Sporting recover from William Salibas opener. Jose Mour inhos Roma earn a comfortable 2-0 win at home against Real Sociedad to put one foot into quarter-finals.\n",
      "11:\n",
      "    Article: WASHINGTON  Calls to pause the development of artificial intelligence (AI) will not solve the challenges ahead, Microsoft co-founder Bill Gates told Reuters, his first public comments since an open letter sparked a debate about the future of the technology. The technologist-turned-philanthropist said that it would be better to focus on how best to use the developments in AI, as it was hard to understand how a pause could work globally. His interview with Reuters comes after an open letter  published on March 29 and co-signed by Mr Elon Musk and more than 1,000 AI experts  demanded an urgent pause in the development of systems more powerful than Microsoft-backed OpenAIs new GPT-4, which can hold human-like conversation, compose songs and summarise lengthy documents. The experts, including Apple co-founder Steve Wozniak, said in the letter that the potential risks and benefits to society need to be assessed. I dont think asking one particular group to pause solves the challenges, Mr Gates said on Monday. Clearly theres huge benefits to these things what we need to do is identify the tricky areas. Microsoft has sought to outpace peers through multi-billion-dollar investments in ChatGPT owner OpenAI. While currently focused full-time on the philanthropic Bill and Melinda Gates Foundation, Mr Gates has been a bullish supporter of AI and described it as revolutionary as the Internet or mobile phones. In a blog titled The Age of AI has begun, which was published and dated March 21, a day before the open letter, he said that he believes AI should be used to help reduce some of the worlds worst inequities. He also said in the interview that the details of any pause would be complicated to enforce. I dont really understand who theyre saying could stop, and would every country in the world agree to stop, and why to stop, he said. But there are a lot of different opinions in this area. \n",
      "    BART Sum.: Bill Gates says it would be better to focus on how best to use the developments in AI. His interview with Reuters comes after an open letter published on March 29. The letter was co-signed by Mr Elon Musk and more than 1,000 AI experts. It demanded an urgent pause in the development of systems more powerful than Microsoft-backed OpenAIs new GPT-4.\n",
      "12:\n",
      "    Article: ROME - Italy said on Friday it was temporarily blocking ChatGPT over data privacy concerns, becoming the first Western country to take such action against the popular artificial intelligence (AI) chatbot. The countrys Data Protection Authority said US company OpenAI, which makes ChatGPT, had no legal basis to justify the mass collection and storage of personal data for the purpose of training the algorithms underlying the operation of the platform.  ChatGPT caused a global sensation when it was released in 2022 for its ability to generate essays, songs, exams and even news articles from brief prompts. But critics have long fretted that it was unclear where ChatGPT and its competitors got their data or how they processed it.  Universities and some education authorities have banned the chatbot over fears that students could use it to write essays or cheat in exams. And hundreds of experts and industry figures signed an open letter this week calling for a pause in the development of powerful AI systems, arguing they posed profound risks to society and humanity. The letter was prompted by OpenAIs release in March of GPT-4, a more powerful version of its chatbot, with even less transparency about its data sources.  OpenAI said on Friday that it has disabled ChatGPT for users in Italy.  We are committed to protecting peoples privacy and we believe we comply with privacy laws. We actively work to reduce personal data in training our AI systems like ChatGPT because we want our AI to learn about the world, not about private individuals, an OpenAI spokesman said.   We also believe that AI regulation is necessary  so we look forward to working closely with (the authorities in Italy) and educating them on how our systems are built and used, the spokesman added. Our users in Italy have told us they find ChatGPT helpful for everyday tasks and we look forward to making it available again soon.  The Italian authority imposed a temporary limitation of the processing of Italian user data by OpenAI and said it had launched an investigation.  In addition to a lack of legal basis for data collection, the authority also highlighted a lack of clarity over whose data was being collected.   It said wrong answers given by the chatbot suggested data was not being handled properly, and accused the company of exposing children to absolutely unsuitable answers.   The watchdog further referenced a data breach on March 20 where user conversations and payment information were compromised  a problem the company blamed on a bug. Professor Nello Cristianini, an AI academic from the University of Bath in Britain, said securing user data and enforcing age limits were easy to fix. But the other two accusations were more problematic  that the model is trained on personal data that is gathered without consent and then not treated properly. It is not clear how these can be fixed any time soon, he said.  The company has been given 20 days to respond and could face a fine of 20 million (S$28.9 million) or up to 4 per cent of annual revenue.  The runaway success of ChatGPT garnered OpenAI a multi-billion-dollar deal with Microsoft, which uses the technology in its Bing search engine and other programs.  It also sparked a gold rush among other tech companies and venture capitalists, with Google hurrying to unveil its own chatbot and investors pouring cash into all manner of AI projects.      \n",
      "    BART Sum.: ChatGPT caused a global sensation when it was released in 2022 for its ability to generate essays, songs, exams and even news articles from brief prompts. Critics have long fretted that it was unclear where ChatGPT and its competitors got their data or how they processed it. Universities and some education authorities have banned the chatbot.\n",
      "13:\n",
      "    Article: SINGAPORE  President Halimah Yacob on Thursday gave her assent to the Governments spending plans for the coming financial year starting on April 1, saying that Budget 2023 is balanced and fiscally responsible. It provides for the immediate needs of Singaporeans, especially with the rising costs of living, but at the same time invests in Singapores future needs, she said in a Facebook post. She noted that significant challenges remain even as the Covid-19 pandemic has abated, with Singapores population rapidly ageing, climate change intensifying, and global security, economic openness and social cohesion coming under threat. The Presidentsassentwas given to the Supply Bill, a move that formally authorises how much the Government can spend in each financial year.  Following her assent, the Bill will be enacted into a law called the Supply Act, which controls the Governments spending in the coming financial year. Madam Halimah said that the Ministry of Finance had earlier briefed her and the Council of Presidential Advisers (CPA) on Budget 2023, and that the Government will not be drawing from the reserves in FY2023. As the Budget is unlikely to draw on the Governments past reserves, and with the CPAs recommendation, I have exercised the Presidents discretionary power under the Constitution to give my assent to the Supply Bill, she added. Deputy Prime Minister and Finance Minister Lawrence Wong had unveiled the $123.7 billion Budget on Feb 14 which included a suite of measures to grow the economy, support parents and families, and support measures to combat the rising cost of living and inflation. Mr Wong had said the Government would not need to draw on past reserves in Budget 2023 as things return to normal. He had noted that there was a lower-than-expected draw on the reserves for Covid-19 emergency public health spending in FY2022.  While Madam Halimah had earlier concurred with a draw of up to $6 billion for this in FY2022, a lower amount of up to $3.1 billion is now expected to be drawn, as the public health situation has since stabilised, said Mr Wong. For FY2020, $31.9 billion from the past reserves was used for Covid-19 response measures, while for FY2021, $5 billion was used.  In all, the draw on the reserves in the three financial years of 2020 to 2022 is expected to come up to $40 billion, less than the initial sum of $52 billion that the Government projected it would need at the onset of the pandemic. Budget 2023 is a balanced and fiscally responsible budget. It provides for the immediate needs of Singaporeans... The President said the past reserves provided critical support to Singaporeans and businesses during Covid-19, enabling jobs to be saved and avoiding severe disruptions to families.  A significant portion of the reserves drawn down was also used to buy vaccines and save lives through ramping up much-needed medical facilities and services, she noted.  In this regard, we see that past reserves were used not just for future generations but also for the current generation that has helped to grow our reserves, she said. In exceptional circumstances, to save lives and livelihoods, our past reserves have provided support across generations of Singaporeans.\n",
      "    BART Sum.: President Halimah Yacob gave her assent to the Governments spending plans for the coming financial year starting on April 1. She said that Budget 2023 is balanced and fiscally responsible. It provides for the immediate needs of Singaporeans, especially with the rising costs of living, but invests in Singapores future needs.\n",
      "14:\n",
      "    Article: LONDON - Finance Minister Jeremy Hunt announced on Wednesday a plan that he hopes will speed up Britains stagnating economy. This includes childcare and tax reforms to get more people into work and corporate tax breaks to boost low levels of business investment. Mr Hunt said the worlds sixth-biggest economy was now expected to avoid a recession this year  even if it is still set to contract. He added he would extend help for households hit by soaring energy bills and freeze a tax on fuel. In the face of enormous challenges, I report today on a British economy which is proving the doubters wrong, Mr Hunt said, to jeers from the opposition Labour Party which is riding high in opinion polls ahead of an election expected next year. In the autumn, we took difficult decisions to deliver stability and sound money, said Mr Hunt, who was rushed into the Treasury last October to undo the plans for tax cuts that sowed chaos in financial markets during Ms Liz Trusss brief premiership. Since mid-October, 10-year gilt rates have fallen, debt servicing costs are down, mortgage rates are lower and inflation has peaked. The International Monetary Fund says our approach means the UK economy is on the right track. After the shocks of Brexit, a heavy Covid-19 hit and double-digit inflation, Britains economy is the only one among Group of Seven nations yet to recover its pre-pandemic size, having already suffered a decade of near-stagnant income growth. Mr Hunt and Prime Minister Rishi Sunak resisted calls from some lawmakers in the ruling Conservative Party for big tax cuts now, focusing instead on the debt rules he announced late last year to calm the chaos in Britains bond markets. But he found money to extend the governments energy bill subsidies for households by a further three months and a decade-long fuel duty freeze by a another year. He also announced a new incentive for business investment that will allow companies to offset 100 per cent of their capital expenditure against profits, although it represented a scaling-back of tax breaks under a previous, two-year scheme. Other measures included more investment in nuclear power. Mr Hunt said the government would add 11 billion (S$18 billion) to the defence budget  which has been stretched by Britains support for Ukraine in its war with Russia  over the next five years. Under a new set of forecasts, gross domestic product (GDP) was set to shrink by 0.2 per cent in 2023 rather than contract by 1.4 per cent as projected by the independent Office for Budget Responsibility (OBR) in November. Since then, energy costs  which soared after Russias invasion of Ukraine  have come down and there have been signs of a recovery in some economic data. Today the Office for Budget Responsibility forecast that because of changing international factors and the measures I take, the UK will not now enter a technical recession this year, Mr Hunt said. The OBR forecast that economic output would grow by 1.8 per cent in 2024 and by 2.5 per cent in 2025, Mr Hunt said, compared with its previous forecasts for growth of 1.3 per cent and 2.6 per cent respectively. Despite continuing global instability, the OBR report today that inflation in the UK will fall from 10.7 per cent in the final quarter of last year to 2.9 per cent by the end of 2023, Mr Hunt said. Many economists have said Mr Hunt probably wants to hold back some fiscal firepower for closer to the next national election. But Wednesdays forecasts underscored the limits on the governments options going forward. They showed that Mr Hunts target to get Britains 2.5 trillion of debt falling as a share of GDP in five years time was on course to be met with a buffer of just 6.5 billion. \n",
      "    BART Sum.: Finance Minister Jeremy Hunt announced a plan that he hopes will speed up Britains stagnating economy. This includes childcare and tax reforms to get more people into work and corporate tax breaks to boost low levels of business investment. Mr Hunt said the worlds sixth-biggest economy was now expected to avoid a recession this year.\n",
      "15:\n",
      "    Article: SINGAPORE  Financial concerns and unclear reporting standards are common barriers that hinder efforts by small and medium-sized enterprises (SMEs) to be more sustainable, according to a study by DBS Bank.  The bank found that 83 per cent of small companies and 92 per cent of medium-sized firms have an environmental, social and governance (ESG) strategy in place or are creating one.  But only 37 per cent of these businesses have a clear road map on how to achieve their goals.  With cash flow being especially critical for these smaller businesses, financial barriers were the most common. Over a third of SMEs pointed out challenges around return on investment, cost of deployment and meeting growth targets, said DBS, South-east Asias largest lender.  Over 800 companies  mostly SMEs  across Singapore, Hong Kong, India, Indonesia, mainland China and Taiwan were studied by the bank last August in partnership with Bloomberg Media Studios.  The firms were from the real estate, mobility, power, agriculture, hospitality, and food and beverage industries.  DBS chief executive Piyush Gupta said: When it comes to SMEs, we understand the many challenges they face transitioning to more sustainable business models. But given that they are the lifeblood of economies, it is imperative that SMEs successfully make the transition.  DBS group chief sustainability officer Helge Muenkel said SMEs increasingly face pressure to turn greener as their supply chain ecosystem often includes large companies that have adopted, or are compelled to adopt, ESG strategies and standards.  Companies that were surveyed said environmental projects will take up the lions share of their sustainability investments.  Most of them added that environmental factors have the greatest impact on their industries, in areas such as waste management, climate change and carbon footprint.  DBS said that incidentally, environmental initiatives are also low-hanging fruits that firms can show to investors and other stakeholders. Access to funding and know-how likewise makes focusing on the E (part of ESG) easier.  But while environmental factors have the biggest impact at the industry level, governance is most likely to influence business decision-making.  Data security, privacy, as well as transparent decision-making and financial reporting are among the most common factors that nudge decisions in one direction or another, said the bank.  Its study also showed that SMEs prioritise initiatives to improve their bottom line, and those that can be immediately adopted.  But these moves might come at the expense of creating ESG frameworks and long-term strategies that take years to fulfil.  A Chinese real estate and construction company said it does not have sufficient resources to investigate and plan ESG initiatives for the next 10 years, with its boss noting: We have our long-term mission but not strategies.  Ms Yulanda Chung, head of sustainability for institutional banking at DBS, said SMEs can start by identifying significant ESG elements to focus on and evaluating whether these factors could improve their bottom line.  For example, an energy-intensive SME could reduce energy costs by using energy-efficient equipment to support decarbonisation. Meanwhile, a labour-intensive SME could consider focusing on the social aspect, and improve the health and safety, or diversity, inclusion and equality of its workforce.  It is also important to analyse industry trends and best practices at an early stage, she added.  This helps firms gauge risks and understand the consequences of failing to align themselves with standards and guidelines.  These exercises can also help them identify opportunities and begin to plan a course of action. Over time, they can turn this into a time-bound, actionable and quantifiable roadmap, said Ms Chung.  SMEs also face a lack of clarity on ESG standards, according to the study. It showed that a third of firms find it tough to get ESG specialists on board, and nearly a third struggle with unclear reporting standards.  This makes it difficult for them to implement projects and measure progress, as well as demonstrate success to stakeholders and investors, said DBS.  Key performance indicators (KPIs) that are simplified and standardised will go a long way towards helping SMEs determine their environmental and social impact, along with metrics based on a companys specific capabilities and profile, said DBS.  It noted: Assessing them by the same criteria as larger companies that have more resources will make it difficult for SMEs to achieve their KPIs.  \n",
      "    BART Sum.: 83% of small companies and 92% of medium-sized firms have an environmental, social and governance (ESG) strategy in place or are creating one. But only 37% of these businesses have a clear road map on how to achieve their goals. Financial concerns and unclear reporting standards are common barriers.\n",
      "16:\n",
      "    Article: NEW YORK  There are more than 170 trillion or 2 million tonnes worth of tiny plastic particles floating on the surface of the ocean, and many of them got there after 2004, according to a paper published on Wednesdayin the journal Plos One. The peer-reviewed paper is by Dr Marcus Eriksen ofthe 5 Gyres Institute, a California-based non-profit organisation focused on plastic pollution, and researchers from other organisations and universities. There was a previous attempt to estimate the amount of plastic afloat on ocean currents back in 2014.  The updated paper relies on data from a greatly expanded set ofsamples  nearly 12,000  from oceans across the globe. For this study,samples werecollected by dragging a net with exceptionally fine mesh for several kilometres across the ocean surface todetermine the average amount of particles per kilometre of water. Then a computer modelanalysedhow plastic concentrates as it leaves rivers, coastlines and shipping lanes. From this, the researchersextrapolated to an estimate of itemsfor the global ocean. Theythen tested their model against real-world concentrations. Taken between 1979 and 2019, the samples reveala rapid and unprecedented increase in ocean plastics since 2005. Dr Eriksen says this wasdriven byseveral factors: adramatic increase inoverall plastic production,  more microplastics (the result of older plastics breaking down over time) and a lack of international laws that dealwith marine pollution. The system is being overwhelmed by all this pollution, he said. We need preventative strategies and not just to focus on clean-up and recycling. We need to find replacements for single use (plastic) because recycling just doesnt work. The oceansupplieshalf the planets oxygen, absorbsmore than a third of carbon dioxide emissions from the burning of fossil fuels and feedsbillions of people.  But it isin trouble from overfishing, plastic dumping and acidification. The report comes just days after negotiators agreed on wording ofa landmark United Nationstreaty with the aim ofconserving30 per cent of the ocean.  That agreement would create protected areas where fishing would be banned and exploitative activities like mining would be limited. However, it would not stop marine plastic pollution, which is largely caused by run-off from the worlds land masses.  Nations have agreed to come up with a framework for a global plastic treaty, but no agreement has been reached.  Thesecond round of UN negotiations on plastic will take place this spring. Meanwhile, most countrieslag even in building the infrastructure that would prevent plastic trash from leaking into the environment.  Last week, the European Investment Bank released a study finding a 6.7 billion (S$9.56 billion) gap in funding going towards the type of sorting and recycling infrastructure that Europe needs to meet its goal of greatly increasing the recycled content in plastic products by 2025.  \n",
      "    BART Sum.: There are more than 170 trillion or 2 million tonnes worth of tiny plastic particles floating on the surface of the ocean. Many of them got there after 2004, according to a paper published on Wednesdayin the journal Plos One. The report comes just days after negotiators agreed on wording of a landmark United Nationstreaty with the aim ofconserving30 per cent of the Ocean.\n",
      "17:\n",
      "    Article: GENEVA  The chief of the World Health Organisation (WHO) pressed China on Thursday to share its information about the origins of Covid-19, saying that until that happened, all hypotheses remained on the table, more than three years after the virus first emerged.  Without full access to the information that China has, you cannot say this or that, said WHO director-general Tedros Adhanom Ghebreyesus in response to a question about the origin of the virus.  All hypotheses are on the table. That is WHOs position and that is why we have been asking China to be cooperative on this. If China does that, then we will know what happened or how it started, he added.  The virus was first reported in the Chinese city of Wuhan in December 2019, with many suspecting that it spread in a live animal market before fanning out around the world and killing nearly seven million people.  Data from the early days of the Covid-19 pandemic was briefly uploaded by Chinese scientists to an international database in March 2023. It included genetic sequences found in more than 1,000 environmental and animal samples taken in January 2020 at the Huanan seafood market in Wuhan, the location of the first known Covid-19 outbreak.  The data showed that DNA from multiple animal species  including raccoon dogs  was present in environmental samples that tested positive for Sars-CoV-2, the virus that causes Covid-19, suggesting that they were the most likely conduits of the disease, according to a team of international researchers. However, in a non-peer-reviewed study published by the Nature journal this week, scientists from Chinas Centre for Disease Control and Prevention disputed the international teams findings.  They said the samples provided no proof that the animals were actually infected.  The samples were also taken a month after human-to-human transmission first occurred at the market, so even if they were Covid-19-positive, the animals could have caught the virus from humans.  Dr Maria Van Kerkhove, the WHOs technical lead for Covid-19, said the latest Chinese information offered some clues on origins but no answers.   She said the United Nations agency was working with scientists to find out more about the earliest cases from 2019, such as the whereabouts of those infected.  She added that the WHO still did not know whether some of the research required had been undertaken in China.   The WHO has also asked the United States for original data that underpinned a recent study by the US Energy Department that suggested a laboratory leak in China had likely caused the Covid-19 pandemic, she said. \n",
      "    BART Sum.: WHO chief Tedros Adhanom Ghebreyesus says all hypotheses are on the table. The virus was first reported in the Chinese city of Wuhan in December 2019. Data from the early days of the Covid-19 pandemic was briefly uploaded by Chinese scientists to an international database in 2023.\n",
      "18:\n",
      "    Article: SINGAPORE - A man who threatened to report to the authorities that he was allowed to dine at a lounge in Marina Bay Sands despite not being fully vaccinated has been sentenced to time behind bars for trying to extort money.  Goh Tian Shun, 30, had lied that he was fully vaccinated and after checks at the lounge showed he was not, Goh threatened to blow up the matter by alerting the media and authorities. The incident happened in August 2021 during a period when only fully vaccinated people were allowed to dine-in at restaurants. On Wednesday, Goh was sentenced to two years and two months  jail and one stroke of the cane. He had pleaded guilty to one extortion charge and three charges for other offences including cheating. The court heard that Goh had kicked up a fuss while at MBS on Aug 14, 2021.  To appease him, MBS food and beverage manager Barry Ng Pak Sen offered him a complimentary meal at the Ruby Lounge after Goh claimed he was fully vaccinated. Deputy Public Prosecutor Tan Yanying said while Goh was eating later that night at about 1am, a staff member asked him again for his vaccination status. This was after a check revealed he was not fully vaccinated. Mr Ng was alerted and approached Goh, who again lied that he was fully vaccinated. Said DPP Tan: When Barry informed the accused that the check via TraceTogether revealed otherwise, the accused became agitated and falsely alleged that the lounge was dirty. Goh eventually admitted he was not fully vaccinated but then said he wanted to file a complaint against MBS.  He eventually left the lounge. At about 5am, he told a shift manager at the integrated resort that the lounge staff had allowed him to dine there despite knowing he was an unvaccinated person.  Goh threatened to report the matter to the authorities and said he would provide them with video footage that he claimed to have. Asked if there was a particular outcome he wanted, Goh demanded compensation for the purported $600,000 he had lost gambling at the casino there since 2018. He said he was willing to settle the matter privately if MBS paid him $200,000 in chips and provided him with a permanent membership upgrade as compensation.  The shift manager said MBS would not compensate the gambling losses but added that the resorts management would be notified of his request. On Aug 16, 2021, Goh sent WhatsApp messages to various casino staff demanding compensation. He said he would alert the media, and claimed he would get former photojournalist Jonathan Choo, who was with The New Paper, to interview casino staff. Goh then sent messages to Mr Ng while pretending to be Mr Choo. At this point, Mr Ng notified his management of the matter and the police were alerted.  In the course of police investigations, Goh admitted that he had impersonated Mr Choo. Goh was given the minimum jail sentence of two years jail for the extortion offence and one stroke of the cane. He could have been jailed for up to five years for the offence.\n",
      "    BART Sum.: Goh Tian Shun threatened to blow up the matter by alerting the media and authorities. The incident happened in August 2021 during a period when only fully vaccinated people were allowed to dine-in at restaurants. Goh was sentenced to two years and two months jail and one stroke of the cane.\n",
      "19:\n",
      "    Article: LONDON  Britains data watchdog said on Tuesday that it has fined TikTok 12.7 million (S$21 million) for breaching data protection laws, including by using the personal data of children under 13 without parental consent. The Information Commissioners Office (ICO) estimated that TikTok allowed as many as 1.4 million British children under 13 to use its platform in 2020, even though it sets 13 as the minimum age to create an account.\n",
      " The ICO said the data breaches occurred between May 2018 and July 2020, with the Chinese-owned video app not having done enough to check who was using the platform and remove the underage children who were on it. There are laws in place to make sure our children are as safe in the digital world as they are in the physical world. TikTok did not abide by those laws, Britains Information Commissioner John Edwards said.\n",
      " Childrens data may have been used to track and profile them, potentially presenting them with harmful or inappropriate content, he added.\n",
      " A TikTok spokesman said the company disagreed with the ICOs decision but was pleased the fine had been reduced from the possible 27 million set out by the ICO in 2022.\n",
      " We invest heavily to help keep under-13s off the platform, and our 40,000-strong safety team works around the clock to help keep the platform safe for our community, the spokesman said.  We will continue to review the decision and are considering next steps. The ICOs fine follows moves by Western governments and institutions in recent weeks, including Britain, to bar the usage of TikTok on official devices over security concerns. \n",
      "    BART Sum.: Britains data watchdog said on Tuesday that it has fined TikTok 12.7 million (S$21 million) for breaching data protection laws. TikTok allowed as many as 1.4 million British children under 13 to use its platform in 2020. The data breaches occurred between May 2018 and July 2020.\n",
      "20:\n",
      "    Article: WASHINGTON - Former US president Donald Trump posted to YouTube and Facebook on Friday, in a return to social media platforms that he used to power his political rise until he was cut off following the Jan 6, 2021, attack on Congress by his followers. Trump shared a video that appeared to be from one of his previous election speeches. Sorry to keep you waiting. Complicated business, Mr Trump was seen saying in the video, captioned IM BACK. Alphabets YouTube restored Trumps channel earlier on Friday. Trump powered his improbable 2016 presidential campaign through his use of social media. His return to the platforms gives him access to key vehicles for political fund-raising, allowing him to reach a combined 146 million followers across three major tech platforms as he makes another run for the presidency in 2024. We carefully evaluated the continued risk of real-world violence, while balancing the chance for voters to hear equally from major national candidates in the run up to an election, YouTube said in a tweet, referring to its move to restore his account. Meta Platforms had reinstated Trumps Facebook and Instagram accounts earlier this year, while his Twitter account was restored in November by the platforms new owner Elon Musk. Mr Trump has yet to post on Twitter. Mr Trumps campaign team did not immediately respond to a request for comment. YouTube banned Mr Trump in 2021 for violating its policy of inciting violence after his supporters stormed the US Capitol when Congress began to certify Joe Bidens victory in the presidential election. I'M BACK! Opponents of Mr Trumps return point to his messages on the Truth Social platform he founded in late 2021, where he has nearly five million followers, as evidence that he still poses the same risk that led various social media platforms to suspend him before. Mr Trumps return to YouTube and Facebook is happening just as the Manhattan District Attorneys office is considering criminal charges related to hush-money payments made to a porn star during Mr Trumps 2016 campaign, charges that Mr Trump and his allies are arguing without evidence are politically motivated. Mr Trump also faces a US$250 million  (S$330 million) civil fraud lawsuit brought by New York state, alleging a decade-long scheme to manipulate more than 200 asset valuations and Mr Trumps net worth to win better terms from banks and insurers. Mr Trump has called the suit a witch hunt.\n",
      "    BART Sum.: Former US president Donald Trump posted to YouTube and Facebook on Friday. His return to the platforms gives him access to key vehicles for political fund-raising. Mr Trump powered his improbable 2016 presidential campaign through his use of social media. YouTube banned Mr Trump in 2021 for violating its policy of inciting violence.\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(df_st_sum['article'])):\n",
    "    print(f\"{idx+1}:\\n    Article: {st_sum_dict['article'][idx]}\\n    BART Sum.: {st_sum_dict['summary_text'][idx]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Simplify the Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for saving the outputs of the simplifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_simplifier_outputs(comp_doc, simp_doc, simp_doc_csv_fp, \n",
    "                            comp_simp_pairs, comp_simp_pairs_fp):\n",
    "    \"\"\"Saves the outputs of the simplifier to the specified CSV files. Also prints the outputs for display\"\"\"\n",
    "\n",
    "    # Save simplified summaries to CSV.\n",
    "    df_simp_doc = pd.DataFrame.from_dict({\n",
    "        \"complex\": comp_doc,\n",
    "        \"simple\": simp_doc\n",
    "    })\n",
    "    df_simp_doc.to_csv(simp_doc_csv_fp, index=False)\n",
    "\n",
    "    # Save the complex-simp sentence pairs to CSv\n",
    "    df_comp_simp_pairs_dict = dict({\n",
    "        \"doc_id\": [],\n",
    "        \"doc_sent_id\": [],\n",
    "        \"complex\": [],\n",
    "        \"simple\": []\n",
    "    })\n",
    "    for doc_idx in range(len(comp_simp_pairs)):\n",
    "        # Get the comp-simp sentence pairs for each doc \n",
    "        doc_sent_pairs = comp_simp_pairs[doc_idx]\n",
    "        for doc_sent_idx, (comp, simp) in enumerate(doc_sent_pairs):\n",
    "            # Store the pair\n",
    "            df_comp_simp_pairs_dict[\"doc_id\"].append(doc_idx)\n",
    "            df_comp_simp_pairs_dict[\"doc_sent_id\"].append(doc_sent_idx)\n",
    "            df_comp_simp_pairs_dict[\"complex\"].append(comp)\n",
    "            df_comp_simp_pairs_dict[\"simple\"].append(simp)\n",
    "    df_comp_simp_pairs = pd.DataFrame.from_dict(df_comp_simp_pairs_dict)\n",
    "    df_comp_simp_pairs.to_csv(comp_simp_pairs_fp, index=False)\n",
    "\n",
    "    # Demo some of the simplifications (at most 5)\n",
    "    for doc_idx in range(min(5, len(comp_simp_pairs))):\n",
    "        print(f\"============================================================\")\n",
    "        print()\n",
    "        \n",
    "        doc_sent_pairs = comp_simp_pairs[doc_idx]\n",
    "        print(f\"Document {doc_idx}:\")\n",
    "        for comp, simp in doc_sent_pairs:\n",
    "            print(f\"Orig: {comp}\")\n",
    "            print(f\"-> Simp: {simp}\")\n",
    "        print()\n",
    "        print(f\"============================================================\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ACCESS to simplify the text in the generated summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     C:\\Users\\hansg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hansg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hansg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from simplertimes import simplify\n",
    "\n",
    "# Create ACCESS simplifier\n",
    "access_simplifier = simplify.create_simplifier(simplify.ACCESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 13:19:55 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'C:\\\\Users\\\\hansg\\\\OneDrive\\\\NUS\\\\Classes\\\\2022-2023-Sem2\\\\CS5246\\\\project\\\\nus_cs5246_project\\\\facebookresearch\\\\access\\\\resources\\\\models\\\\best_model\\\\checkpoints\\\\checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': \"{'encoder_embed_path': None, 'decoder_embed_path': None}\", 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'raw', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'tmp', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 'hard', 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'C:\\\\Users\\\\hansg\\\\AppData\\\\Local\\\\Temp\\\\tmpad9nx429', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': 'raw', 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-04-16 13:19:55 | INFO | fairseq.tasks.translation | [complex] dictionary: 10144 types\n",
      "2023-04-16 13:19:55 | INFO | fairseq.tasks.translation | [simple] dictionary: 10048 types\n",
      "2023-04-16 13:19:55 | INFO | fairseq_cli.generate | loading model(s) from C:\\Users\\hansg\\OneDrive\\NUS\\Classes\\2022-2023-Sem2\\CS5246\\project\\nus_cs5246_project\\facebookresearch\\access\\resources\\models\\best_model\\checkpoints\\checkpoint_best.pt\n",
      "2023-04-16 13:20:03 | INFO | fairseq.data.data_utils | loaded 4 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpad9nx429\\tmp.complex-simple.complex\n",
      "2023-04-16 13:20:03 | INFO | fairseq.data.data_utils | loaded 4 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpad9nx429\\tmp.complex-simple.simple\n",
      "2023-04-16 13:20:03 | INFO | fairseq.tasks.translation | C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpad9nx429 tmp complex-simple 4 examples\n",
      "2023-04-16 13:20:20 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-04-16 13:20:20 | INFO | fairseq_cli.generate | Translated 4 sentences (74 tokens) in 8.1s (0.49 sentences/s, 9.12 tokens/s)\n",
      "2023-04-16 13:20:22 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'C:\\\\Users\\\\hansg\\\\OneDrive\\\\NUS\\\\Classes\\\\2022-2023-Sem2\\\\CS5246\\\\project\\\\nus_cs5246_project\\\\facebookresearch\\\\access\\\\resources\\\\models\\\\best_model\\\\checkpoints\\\\checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': \"{'encoder_embed_path': None, 'decoder_embed_path': None}\", 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'raw', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'tmp', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 'hard', 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'C:\\\\Users\\\\hansg\\\\AppData\\\\Local\\\\Temp\\\\tmp426zizi3', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': 'raw', 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-04-16 13:20:22 | INFO | fairseq.tasks.translation | [complex] dictionary: 10144 types\n",
      "2023-04-16 13:20:22 | INFO | fairseq.tasks.translation | [simple] dictionary: 10048 types\n",
      "2023-04-16 13:20:22 | INFO | fairseq_cli.generate | loading model(s) from C:\\Users\\hansg\\OneDrive\\NUS\\Classes\\2022-2023-Sem2\\CS5246\\project\\nus_cs5246_project\\facebookresearch\\access\\resources\\models\\best_model\\checkpoints\\checkpoint_best.pt\n",
      "2023-04-16 13:20:31 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmp426zizi3\\tmp.complex-simple.complex\n",
      "2023-04-16 13:20:31 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmp426zizi3\\tmp.complex-simple.simple\n",
      "2023-04-16 13:20:31 | INFO | fairseq.tasks.translation | C:\\Users\\hansg\\AppData\\Local\\Temp\\tmp426zizi3 tmp complex-simple 3 examples\n",
      "2023-04-16 13:20:33 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-04-16 13:20:33 | INFO | fairseq_cli.generate | Translated 3 sentences (69 tokens) in 0.5s (5.98 sentences/s, 137.49 tokens/s)\n",
      "2023-04-16 13:20:36 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'C:\\\\Users\\\\hansg\\\\OneDrive\\\\NUS\\\\Classes\\\\2022-2023-Sem2\\\\CS5246\\\\project\\\\nus_cs5246_project\\\\facebookresearch\\\\access\\\\resources\\\\models\\\\best_model\\\\checkpoints\\\\checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': \"{'encoder_embed_path': None, 'decoder_embed_path': None}\", 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'raw', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'tmp', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 'hard', 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'C:\\\\Users\\\\hansg\\\\AppData\\\\Local\\\\Temp\\\\tmp5oj7dxqm', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': 'raw', 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-04-16 13:20:36 | INFO | fairseq.tasks.translation | [complex] dictionary: 10144 types\n",
      "2023-04-16 13:20:36 | INFO | fairseq.tasks.translation | [simple] dictionary: 10048 types\n",
      "2023-04-16 13:20:36 | INFO | fairseq_cli.generate | loading model(s) from C:\\Users\\hansg\\OneDrive\\NUS\\Classes\\2022-2023-Sem2\\CS5246\\project\\nus_cs5246_project\\facebookresearch\\access\\resources\\models\\best_model\\checkpoints\\checkpoint_best.pt\n",
      "2023-04-16 13:20:43 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmp5oj7dxqm\\tmp.complex-simple.complex\n",
      "2023-04-16 13:20:43 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmp5oj7dxqm\\tmp.complex-simple.simple\n",
      "2023-04-16 13:20:43 | INFO | fairseq.tasks.translation | C:\\Users\\hansg\\AppData\\Local\\Temp\\tmp5oj7dxqm tmp complex-simple 3 examples\n",
      "2023-04-16 13:20:45 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-04-16 13:20:45 | INFO | fairseq_cli.generate | Translated 3 sentences (70 tokens) in 0.6s (4.84 sentences/s, 112.86 tokens/s)\n",
      "2023-04-16 13:20:47 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'C:\\\\Users\\\\hansg\\\\OneDrive\\\\NUS\\\\Classes\\\\2022-2023-Sem2\\\\CS5246\\\\project\\\\nus_cs5246_project\\\\facebookresearch\\\\access\\\\resources\\\\models\\\\best_model\\\\checkpoints\\\\checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': \"{'encoder_embed_path': None, 'decoder_embed_path': None}\", 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'raw', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'tmp', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 'hard', 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'C:\\\\Users\\\\hansg\\\\AppData\\\\Local\\\\Temp\\\\tmpqnkxi8at', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': 'raw', 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-04-16 13:20:47 | INFO | fairseq.tasks.translation | [complex] dictionary: 10144 types\n",
      "2023-04-16 13:20:47 | INFO | fairseq.tasks.translation | [simple] dictionary: 10048 types\n",
      "2023-04-16 13:20:47 | INFO | fairseq_cli.generate | loading model(s) from C:\\Users\\hansg\\OneDrive\\NUS\\Classes\\2022-2023-Sem2\\CS5246\\project\\nus_cs5246_project\\facebookresearch\\access\\resources\\models\\best_model\\checkpoints\\checkpoint_best.pt\n",
      "2023-04-16 13:20:52 | INFO | fairseq.data.data_utils | loaded 4 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpqnkxi8at\\tmp.complex-simple.complex\n",
      "2023-04-16 13:20:52 | INFO | fairseq.data.data_utils | loaded 4 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpqnkxi8at\\tmp.complex-simple.simple\n",
      "2023-04-16 13:20:52 | INFO | fairseq.tasks.translation | C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpqnkxi8at tmp complex-simple 4 examples\n",
      "2023-04-16 13:20:53 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-04-16 13:20:53 | INFO | fairseq_cli.generate | Translated 4 sentences (75 tokens) in 0.4s (11.43 sentences/s, 214.28 tokens/s)\n",
      "2023-04-16 13:20:55 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'C:\\\\Users\\\\hansg\\\\OneDrive\\\\NUS\\\\Classes\\\\2022-2023-Sem2\\\\CS5246\\\\project\\\\nus_cs5246_project\\\\facebookresearch\\\\access\\\\resources\\\\models\\\\best_model\\\\checkpoints\\\\checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': \"{'encoder_embed_path': None, 'decoder_embed_path': None}\", 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'raw', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'tmp', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 'hard', 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'C:\\\\Users\\\\hansg\\\\AppData\\\\Local\\\\Temp\\\\tmph61isz8t', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': 'raw', 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-04-16 13:20:55 | INFO | fairseq.tasks.translation | [complex] dictionary: 10144 types\n",
      "2023-04-16 13:20:55 | INFO | fairseq.tasks.translation | [simple] dictionary: 10048 types\n",
      "2023-04-16 13:20:55 | INFO | fairseq_cli.generate | loading model(s) from C:\\Users\\hansg\\OneDrive\\NUS\\Classes\\2022-2023-Sem2\\CS5246\\project\\nus_cs5246_project\\facebookresearch\\access\\resources\\models\\best_model\\checkpoints\\checkpoint_best.pt\n",
      "2023-04-16 13:21:00 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmph61isz8t\\tmp.complex-simple.complex\n",
      "2023-04-16 13:21:00 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmph61isz8t\\tmp.complex-simple.simple\n",
      "2023-04-16 13:21:00 | INFO | fairseq.tasks.translation | C:\\Users\\hansg\\AppData\\Local\\Temp\\tmph61isz8t tmp complex-simple 3 examples\n",
      "2023-04-16 13:21:00 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-04-16 13:21:00 | INFO | fairseq_cli.generate | Translated 3 sentences (60 tokens) in 0.3s (8.94 sentences/s, 178.84 tokens/s)\n",
      "2023-04-16 13:21:02 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'C:\\\\Users\\\\hansg\\\\OneDrive\\\\NUS\\\\Classes\\\\2022-2023-Sem2\\\\CS5246\\\\project\\\\nus_cs5246_project\\\\facebookresearch\\\\access\\\\resources\\\\models\\\\best_model\\\\checkpoints\\\\checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': \"{'encoder_embed_path': None, 'decoder_embed_path': None}\", 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'raw', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'tmp', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 'hard', 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'C:\\\\Users\\\\hansg\\\\AppData\\\\Local\\\\Temp\\\\tmphy5gdrok', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': 'raw', 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-04-16 13:21:02 | INFO | fairseq.tasks.translation | [complex] dictionary: 10144 types\n",
      "2023-04-16 13:21:02 | INFO | fairseq.tasks.translation | [simple] dictionary: 10048 types\n",
      "2023-04-16 13:21:02 | INFO | fairseq_cli.generate | loading model(s) from C:\\Users\\hansg\\OneDrive\\NUS\\Classes\\2022-2023-Sem2\\CS5246\\project\\nus_cs5246_project\\facebookresearch\\access\\resources\\models\\best_model\\checkpoints\\checkpoint_best.pt\n",
      "2023-04-16 13:21:07 | INFO | fairseq.data.data_utils | loaded 4 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmphy5gdrok\\tmp.complex-simple.complex\n",
      "2023-04-16 13:21:07 | INFO | fairseq.data.data_utils | loaded 4 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmphy5gdrok\\tmp.complex-simple.simple\n",
      "2023-04-16 13:21:07 | INFO | fairseq.tasks.translation | C:\\Users\\hansg\\AppData\\Local\\Temp\\tmphy5gdrok tmp complex-simple 4 examples\n",
      "2023-04-16 13:21:08 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-04-16 13:21:08 | INFO | fairseq_cli.generate | Translated 4 sentences (81 tokens) in 0.4s (10.32 sentences/s, 209.02 tokens/s)\n",
      "2023-04-16 13:21:10 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'C:\\\\Users\\\\hansg\\\\OneDrive\\\\NUS\\\\Classes\\\\2022-2023-Sem2\\\\CS5246\\\\project\\\\nus_cs5246_project\\\\facebookresearch\\\\access\\\\resources\\\\models\\\\best_model\\\\checkpoints\\\\checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': \"{'encoder_embed_path': None, 'decoder_embed_path': None}\", 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'raw', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'tmp', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 'hard', 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'C:\\\\Users\\\\hansg\\\\AppData\\\\Local\\\\Temp\\\\tmpkf9dgcbc', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': 'raw', 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-04-16 13:21:10 | INFO | fairseq.tasks.translation | [complex] dictionary: 10144 types\n",
      "2023-04-16 13:21:10 | INFO | fairseq.tasks.translation | [simple] dictionary: 10048 types\n",
      "2023-04-16 13:21:10 | INFO | fairseq_cli.generate | loading model(s) from C:\\Users\\hansg\\OneDrive\\NUS\\Classes\\2022-2023-Sem2\\CS5246\\project\\nus_cs5246_project\\facebookresearch\\access\\resources\\models\\best_model\\checkpoints\\checkpoint_best.pt\n",
      "2023-04-16 13:21:15 | INFO | fairseq.data.data_utils | loaded 4 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpkf9dgcbc\\tmp.complex-simple.complex\n",
      "2023-04-16 13:21:15 | INFO | fairseq.data.data_utils | loaded 4 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpkf9dgcbc\\tmp.complex-simple.simple\n",
      "2023-04-16 13:21:15 | INFO | fairseq.tasks.translation | C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpkf9dgcbc tmp complex-simple 4 examples\n",
      "2023-04-16 13:21:16 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-04-16 13:21:16 | INFO | fairseq_cli.generate | Translated 4 sentences (70 tokens) in 0.4s (10.34 sentences/s, 181.03 tokens/s)\n",
      "2023-04-16 13:21:19 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'C:\\\\Users\\\\hansg\\\\OneDrive\\\\NUS\\\\Classes\\\\2022-2023-Sem2\\\\CS5246\\\\project\\\\nus_cs5246_project\\\\facebookresearch\\\\access\\\\resources\\\\models\\\\best_model\\\\checkpoints\\\\checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': \"{'encoder_embed_path': None, 'decoder_embed_path': None}\", 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'raw', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'tmp', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 'hard', 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'C:\\\\Users\\\\hansg\\\\AppData\\\\Local\\\\Temp\\\\tmpsm3axhvr', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': 'raw', 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-04-16 13:21:19 | INFO | fairseq.tasks.translation | [complex] dictionary: 10144 types\n",
      "2023-04-16 13:21:19 | INFO | fairseq.tasks.translation | [simple] dictionary: 10048 types\n",
      "2023-04-16 13:21:19 | INFO | fairseq_cli.generate | loading model(s) from C:\\Users\\hansg\\OneDrive\\NUS\\Classes\\2022-2023-Sem2\\CS5246\\project\\nus_cs5246_project\\facebookresearch\\access\\resources\\models\\best_model\\checkpoints\\checkpoint_best.pt\n",
      "2023-04-16 13:21:24 | INFO | fairseq.data.data_utils | loaded 4 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpsm3axhvr\\tmp.complex-simple.complex\n",
      "2023-04-16 13:21:24 | INFO | fairseq.data.data_utils | loaded 4 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpsm3axhvr\\tmp.complex-simple.simple\n",
      "2023-04-16 13:21:24 | INFO | fairseq.tasks.translation | C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpsm3axhvr tmp complex-simple 4 examples\n",
      "2023-04-16 13:21:25 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-04-16 13:21:25 | INFO | fairseq_cli.generate | Translated 4 sentences (82 tokens) in 0.4s (9.62 sentences/s, 197.26 tokens/s)\n",
      "2023-04-16 13:21:27 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'C:\\\\Users\\\\hansg\\\\OneDrive\\\\NUS\\\\Classes\\\\2022-2023-Sem2\\\\CS5246\\\\project\\\\nus_cs5246_project\\\\facebookresearch\\\\access\\\\resources\\\\models\\\\best_model\\\\checkpoints\\\\checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': \"{'encoder_embed_path': None, 'decoder_embed_path': None}\", 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'raw', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'tmp', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 'hard', 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'C:\\\\Users\\\\hansg\\\\AppData\\\\Local\\\\Temp\\\\tmpgqd0iz19', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': 'raw', 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-04-16 13:21:27 | INFO | fairseq.tasks.translation | [complex] dictionary: 10144 types\n",
      "2023-04-16 13:21:27 | INFO | fairseq.tasks.translation | [simple] dictionary: 10048 types\n",
      "2023-04-16 13:21:27 | INFO | fairseq_cli.generate | loading model(s) from C:\\Users\\hansg\\OneDrive\\NUS\\Classes\\2022-2023-Sem2\\CS5246\\project\\nus_cs5246_project\\facebookresearch\\access\\resources\\models\\best_model\\checkpoints\\checkpoint_best.pt\n",
      "2023-04-16 13:21:32 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpgqd0iz19\\tmp.complex-simple.complex\n",
      "2023-04-16 13:21:32 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpgqd0iz19\\tmp.complex-simple.simple\n",
      "2023-04-16 13:21:32 | INFO | fairseq.tasks.translation | C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpgqd0iz19 tmp complex-simple 3 examples\n",
      "2023-04-16 13:21:33 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-04-16 13:21:33 | INFO | fairseq_cli.generate | Translated 3 sentences (81 tokens) in 0.6s (5.39 sentences/s, 145.45 tokens/s)\n",
      "2023-04-16 13:21:35 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'C:\\\\Users\\\\hansg\\\\OneDrive\\\\NUS\\\\Classes\\\\2022-2023-Sem2\\\\CS5246\\\\project\\\\nus_cs5246_project\\\\facebookresearch\\\\access\\\\resources\\\\models\\\\best_model\\\\checkpoints\\\\checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': \"{'encoder_embed_path': None, 'decoder_embed_path': None}\", 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'raw', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'tmp', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 'hard', 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'C:\\\\Users\\\\hansg\\\\AppData\\\\Local\\\\Temp\\\\tmpmu3pl3tq', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': 'raw', 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-04-16 13:21:35 | INFO | fairseq.tasks.translation | [complex] dictionary: 10144 types\n",
      "2023-04-16 13:21:35 | INFO | fairseq.tasks.translation | [simple] dictionary: 10048 types\n",
      "2023-04-16 13:21:35 | INFO | fairseq_cli.generate | loading model(s) from C:\\Users\\hansg\\OneDrive\\NUS\\Classes\\2022-2023-Sem2\\CS5246\\project\\nus_cs5246_project\\facebookresearch\\access\\resources\\models\\best_model\\checkpoints\\checkpoint_best.pt\n",
      "2023-04-16 13:21:40 | INFO | fairseq.data.data_utils | loaded 4 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpmu3pl3tq\\tmp.complex-simple.complex\n",
      "2023-04-16 13:21:40 | INFO | fairseq.data.data_utils | loaded 4 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpmu3pl3tq\\tmp.complex-simple.simple\n",
      "2023-04-16 13:21:40 | INFO | fairseq.tasks.translation | C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpmu3pl3tq tmp complex-simple 4 examples\n",
      "2023-04-16 13:21:41 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-04-16 13:21:41 | INFO | fairseq_cli.generate | Translated 4 sentences (98 tokens) in 0.7s (6.09 sentences/s, 149.28 tokens/s)\n",
      "2023-04-16 13:21:43 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'C:\\\\Users\\\\hansg\\\\OneDrive\\\\NUS\\\\Classes\\\\2022-2023-Sem2\\\\CS5246\\\\project\\\\nus_cs5246_project\\\\facebookresearch\\\\access\\\\resources\\\\models\\\\best_model\\\\checkpoints\\\\checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': \"{'encoder_embed_path': None, 'decoder_embed_path': None}\", 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'raw', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'tmp', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 'hard', 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'C:\\\\Users\\\\hansg\\\\AppData\\\\Local\\\\Temp\\\\tmpoumn4gcc', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': 'raw', 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-04-16 13:21:44 | INFO | fairseq.tasks.translation | [complex] dictionary: 10144 types\n",
      "2023-04-16 13:21:44 | INFO | fairseq.tasks.translation | [simple] dictionary: 10048 types\n",
      "2023-04-16 13:21:44 | INFO | fairseq_cli.generate | loading model(s) from C:\\Users\\hansg\\OneDrive\\NUS\\Classes\\2022-2023-Sem2\\CS5246\\project\\nus_cs5246_project\\facebookresearch\\access\\resources\\models\\best_model\\checkpoints\\checkpoint_best.pt\n",
      "2023-04-16 13:21:49 | INFO | fairseq.data.data_utils | loaded 4 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpoumn4gcc\\tmp.complex-simple.complex\n",
      "2023-04-16 13:21:49 | INFO | fairseq.data.data_utils | loaded 4 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpoumn4gcc\\tmp.complex-simple.simple\n",
      "2023-04-16 13:21:49 | INFO | fairseq.tasks.translation | C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpoumn4gcc tmp complex-simple 4 examples\n",
      "2023-04-16 13:21:50 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-04-16 13:21:50 | INFO | fairseq_cli.generate | Translated 4 sentences (88 tokens) in 0.5s (7.68 sentences/s, 168.89 tokens/s)\n",
      "2023-04-16 13:21:52 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'C:\\\\Users\\\\hansg\\\\OneDrive\\\\NUS\\\\Classes\\\\2022-2023-Sem2\\\\CS5246\\\\project\\\\nus_cs5246_project\\\\facebookresearch\\\\access\\\\resources\\\\models\\\\best_model\\\\checkpoints\\\\checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': \"{'encoder_embed_path': None, 'decoder_embed_path': None}\", 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'raw', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'tmp', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 'hard', 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'C:\\\\Users\\\\hansg\\\\AppData\\\\Local\\\\Temp\\\\tmpgt_wlwjc', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': 'raw', 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-04-16 13:21:52 | INFO | fairseq.tasks.translation | [complex] dictionary: 10144 types\n",
      "2023-04-16 13:21:52 | INFO | fairseq.tasks.translation | [simple] dictionary: 10048 types\n",
      "2023-04-16 13:21:52 | INFO | fairseq_cli.generate | loading model(s) from C:\\Users\\hansg\\OneDrive\\NUS\\Classes\\2022-2023-Sem2\\CS5246\\project\\nus_cs5246_project\\facebookresearch\\access\\resources\\models\\best_model\\checkpoints\\checkpoint_best.pt\n",
      "2023-04-16 13:21:57 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpgt_wlwjc\\tmp.complex-simple.complex\n",
      "2023-04-16 13:21:57 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpgt_wlwjc\\tmp.complex-simple.simple\n",
      "2023-04-16 13:21:57 | INFO | fairseq.tasks.translation | C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpgt_wlwjc tmp complex-simple 3 examples\n",
      "2023-04-16 13:21:58 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-04-16 13:21:58 | INFO | fairseq_cli.generate | Translated 3 sentences (88 tokens) in 0.7s (4.45 sentences/s, 130.62 tokens/s)\n",
      "2023-04-16 13:22:01 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'C:\\\\Users\\\\hansg\\\\OneDrive\\\\NUS\\\\Classes\\\\2022-2023-Sem2\\\\CS5246\\\\project\\\\nus_cs5246_project\\\\facebookresearch\\\\access\\\\resources\\\\models\\\\best_model\\\\checkpoints\\\\checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': \"{'encoder_embed_path': None, 'decoder_embed_path': None}\", 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'raw', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'tmp', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 'hard', 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'C:\\\\Users\\\\hansg\\\\AppData\\\\Local\\\\Temp\\\\tmpfezoorb9', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': 'raw', 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-04-16 13:22:01 | INFO | fairseq.tasks.translation | [complex] dictionary: 10144 types\n",
      "2023-04-16 13:22:01 | INFO | fairseq.tasks.translation | [simple] dictionary: 10048 types\n",
      "2023-04-16 13:22:01 | INFO | fairseq_cli.generate | loading model(s) from C:\\Users\\hansg\\OneDrive\\NUS\\Classes\\2022-2023-Sem2\\CS5246\\project\\nus_cs5246_project\\facebookresearch\\access\\resources\\models\\best_model\\checkpoints\\checkpoint_best.pt\n",
      "2023-04-16 13:22:06 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpfezoorb9\\tmp.complex-simple.complex\n",
      "2023-04-16 13:22:06 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpfezoorb9\\tmp.complex-simple.simple\n",
      "2023-04-16 13:22:06 | INFO | fairseq.tasks.translation | C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpfezoorb9 tmp complex-simple 3 examples\n",
      "2023-04-16 13:22:07 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-04-16 13:22:07 | INFO | fairseq_cli.generate | Translated 3 sentences (71 tokens) in 0.5s (6.26 sentences/s, 148.12 tokens/s)\n",
      "2023-04-16 13:22:09 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'C:\\\\Users\\\\hansg\\\\OneDrive\\\\NUS\\\\Classes\\\\2022-2023-Sem2\\\\CS5246\\\\project\\\\nus_cs5246_project\\\\facebookresearch\\\\access\\\\resources\\\\models\\\\best_model\\\\checkpoints\\\\checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': \"{'encoder_embed_path': None, 'decoder_embed_path': None}\", 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'raw', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'tmp', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 'hard', 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'C:\\\\Users\\\\hansg\\\\AppData\\\\Local\\\\Temp\\\\tmp2gyxga9s', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': 'raw', 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-04-16 13:22:09 | INFO | fairseq.tasks.translation | [complex] dictionary: 10144 types\n",
      "2023-04-16 13:22:09 | INFO | fairseq.tasks.translation | [simple] dictionary: 10048 types\n",
      "2023-04-16 13:22:09 | INFO | fairseq_cli.generate | loading model(s) from C:\\Users\\hansg\\OneDrive\\NUS\\Classes\\2022-2023-Sem2\\CS5246\\project\\nus_cs5246_project\\facebookresearch\\access\\resources\\models\\best_model\\checkpoints\\checkpoint_best.pt\n",
      "2023-04-16 13:22:15 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmp2gyxga9s\\tmp.complex-simple.complex\n",
      "2023-04-16 13:22:15 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmp2gyxga9s\\tmp.complex-simple.simple\n",
      "2023-04-16 13:22:15 | INFO | fairseq.tasks.translation | C:\\Users\\hansg\\AppData\\Local\\Temp\\tmp2gyxga9s tmp complex-simple 3 examples\n",
      "2023-04-16 13:22:16 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-04-16 13:22:16 | INFO | fairseq_cli.generate | Translated 3 sentences (74 tokens) in 0.6s (4.62 sentences/s, 114.02 tokens/s)\n",
      "2023-04-16 13:22:18 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'C:\\\\Users\\\\hansg\\\\OneDrive\\\\NUS\\\\Classes\\\\2022-2023-Sem2\\\\CS5246\\\\project\\\\nus_cs5246_project\\\\facebookresearch\\\\access\\\\resources\\\\models\\\\best_model\\\\checkpoints\\\\checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': \"{'encoder_embed_path': None, 'decoder_embed_path': None}\", 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'raw', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'tmp', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 'hard', 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'C:\\\\Users\\\\hansg\\\\AppData\\\\Local\\\\Temp\\\\tmpqoc5tbrp', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': 'raw', 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-04-16 13:22:18 | INFO | fairseq.tasks.translation | [complex] dictionary: 10144 types\n",
      "2023-04-16 13:22:18 | INFO | fairseq.tasks.translation | [simple] dictionary: 10048 types\n",
      "2023-04-16 13:22:18 | INFO | fairseq_cli.generate | loading model(s) from C:\\Users\\hansg\\OneDrive\\NUS\\Classes\\2022-2023-Sem2\\CS5246\\project\\nus_cs5246_project\\facebookresearch\\access\\resources\\models\\best_model\\checkpoints\\checkpoint_best.pt\n",
      "2023-04-16 13:22:23 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpqoc5tbrp\\tmp.complex-simple.complex\n",
      "2023-04-16 13:22:23 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpqoc5tbrp\\tmp.complex-simple.simple\n",
      "2023-04-16 13:22:23 | INFO | fairseq.tasks.translation | C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpqoc5tbrp tmp complex-simple 3 examples\n",
      "2023-04-16 13:22:24 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-04-16 13:22:24 | INFO | fairseq_cli.generate | Translated 3 sentences (72 tokens) in 0.6s (5.17 sentences/s, 124.15 tokens/s)\n",
      "2023-04-16 13:22:26 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'C:\\\\Users\\\\hansg\\\\OneDrive\\\\NUS\\\\Classes\\\\2022-2023-Sem2\\\\CS5246\\\\project\\\\nus_cs5246_project\\\\facebookresearch\\\\access\\\\resources\\\\models\\\\best_model\\\\checkpoints\\\\checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': \"{'encoder_embed_path': None, 'decoder_embed_path': None}\", 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'raw', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'tmp', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 'hard', 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'C:\\\\Users\\\\hansg\\\\AppData\\\\Local\\\\Temp\\\\tmp4c4udkvm', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': 'raw', 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-04-16 13:22:26 | INFO | fairseq.tasks.translation | [complex] dictionary: 10144 types\n",
      "2023-04-16 13:22:26 | INFO | fairseq.tasks.translation | [simple] dictionary: 10048 types\n",
      "2023-04-16 13:22:26 | INFO | fairseq_cli.generate | loading model(s) from C:\\Users\\hansg\\OneDrive\\NUS\\Classes\\2022-2023-Sem2\\CS5246\\project\\nus_cs5246_project\\facebookresearch\\access\\resources\\models\\best_model\\checkpoints\\checkpoint_best.pt\n",
      "2023-04-16 13:22:32 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmp4c4udkvm\\tmp.complex-simple.complex\n",
      "2023-04-16 13:22:32 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmp4c4udkvm\\tmp.complex-simple.simple\n",
      "2023-04-16 13:22:32 | INFO | fairseq.tasks.translation | C:\\Users\\hansg\\AppData\\Local\\Temp\\tmp4c4udkvm tmp complex-simple 3 examples\n",
      "2023-04-16 13:22:33 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-04-16 13:22:33 | INFO | fairseq_cli.generate | Translated 3 sentences (88 tokens) in 0.7s (4.58 sentences/s, 134.32 tokens/s)\n",
      "2023-04-16 13:22:35 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'C:\\\\Users\\\\hansg\\\\OneDrive\\\\NUS\\\\Classes\\\\2022-2023-Sem2\\\\CS5246\\\\project\\\\nus_cs5246_project\\\\facebookresearch\\\\access\\\\resources\\\\models\\\\best_model\\\\checkpoints\\\\checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': \"{'encoder_embed_path': None, 'decoder_embed_path': None}\", 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'raw', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'tmp', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 'hard', 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'C:\\\\Users\\\\hansg\\\\AppData\\\\Local\\\\Temp\\\\tmpgv3s5sv2', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': 'raw', 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-04-16 13:22:35 | INFO | fairseq.tasks.translation | [complex] dictionary: 10144 types\n",
      "2023-04-16 13:22:35 | INFO | fairseq.tasks.translation | [simple] dictionary: 10048 types\n",
      "2023-04-16 13:22:35 | INFO | fairseq_cli.generate | loading model(s) from C:\\Users\\hansg\\OneDrive\\NUS\\Classes\\2022-2023-Sem2\\CS5246\\project\\nus_cs5246_project\\facebookresearch\\access\\resources\\models\\best_model\\checkpoints\\checkpoint_best.pt\n",
      "2023-04-16 13:22:40 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpgv3s5sv2\\tmp.complex-simple.complex\n",
      "2023-04-16 13:22:40 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpgv3s5sv2\\tmp.complex-simple.simple\n",
      "2023-04-16 13:22:40 | INFO | fairseq.tasks.translation | C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpgv3s5sv2 tmp complex-simple 3 examples\n",
      "2023-04-16 13:22:41 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-04-16 13:22:41 | INFO | fairseq_cli.generate | Translated 3 sentences (75 tokens) in 0.4s (6.85 sentences/s, 171.27 tokens/s)\n",
      "2023-04-16 13:22:43 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'C:\\\\Users\\\\hansg\\\\OneDrive\\\\NUS\\\\Classes\\\\2022-2023-Sem2\\\\CS5246\\\\project\\\\nus_cs5246_project\\\\facebookresearch\\\\access\\\\resources\\\\models\\\\best_model\\\\checkpoints\\\\checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': \"{'encoder_embed_path': None, 'decoder_embed_path': None}\", 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'raw', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'tmp', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 'hard', 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'C:\\\\Users\\\\hansg\\\\AppData\\\\Local\\\\Temp\\\\tmpn5dwx7q5', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': 'raw', 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-04-16 13:22:43 | INFO | fairseq.tasks.translation | [complex] dictionary: 10144 types\n",
      "2023-04-16 13:22:43 | INFO | fairseq.tasks.translation | [simple] dictionary: 10048 types\n",
      "2023-04-16 13:22:43 | INFO | fairseq_cli.generate | loading model(s) from C:\\Users\\hansg\\OneDrive\\NUS\\Classes\\2022-2023-Sem2\\CS5246\\project\\nus_cs5246_project\\facebookresearch\\access\\resources\\models\\best_model\\checkpoints\\checkpoint_best.pt\n",
      "2023-04-16 13:22:48 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpn5dwx7q5\\tmp.complex-simple.complex\n",
      "2023-04-16 13:22:48 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpn5dwx7q5\\tmp.complex-simple.simple\n",
      "2023-04-16 13:22:48 | INFO | fairseq.tasks.translation | C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpn5dwx7q5 tmp complex-simple 3 examples\n",
      "2023-04-16 13:22:49 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-04-16 13:22:49 | INFO | fairseq_cli.generate | Translated 3 sentences (66 tokens) in 0.4s (7.39 sentences/s, 162.63 tokens/s)\n",
      "2023-04-16 13:22:51 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'C:\\\\Users\\\\hansg\\\\OneDrive\\\\NUS\\\\Classes\\\\2022-2023-Sem2\\\\CS5246\\\\project\\\\nus_cs5246_project\\\\facebookresearch\\\\access\\\\resources\\\\models\\\\best_model\\\\checkpoints\\\\checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': \"{'encoder_embed_path': None, 'decoder_embed_path': None}\", 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'raw', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'tmp', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 'hard', 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'C:\\\\Users\\\\hansg\\\\AppData\\\\Local\\\\Temp\\\\tmpgr74ahpd', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': 'raw', 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-04-16 13:22:51 | INFO | fairseq.tasks.translation | [complex] dictionary: 10144 types\n",
      "2023-04-16 13:22:51 | INFO | fairseq.tasks.translation | [simple] dictionary: 10048 types\n",
      "2023-04-16 13:22:51 | INFO | fairseq_cli.generate | loading model(s) from C:\\Users\\hansg\\OneDrive\\NUS\\Classes\\2022-2023-Sem2\\CS5246\\project\\nus_cs5246_project\\facebookresearch\\access\\resources\\models\\best_model\\checkpoints\\checkpoint_best.pt\n",
      "2023-04-16 13:22:57 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpgr74ahpd\\tmp.complex-simple.complex\n",
      "2023-04-16 13:22:57 | INFO | fairseq.data.data_utils | loaded 3 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpgr74ahpd\\tmp.complex-simple.simple\n",
      "2023-04-16 13:22:57 | INFO | fairseq.tasks.translation | C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpgr74ahpd tmp complex-simple 3 examples\n",
      "2023-04-16 13:22:58 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-04-16 13:22:58 | INFO | fairseq_cli.generate | Translated 3 sentences (70 tokens) in 0.6s (4.87 sentences/s, 113.68 tokens/s)\n",
      "2023-04-16 13:23:00 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'C:\\\\Users\\\\hansg\\\\OneDrive\\\\NUS\\\\Classes\\\\2022-2023-Sem2\\\\CS5246\\\\project\\\\nus_cs5246_project\\\\facebookresearch\\\\access\\\\resources\\\\models\\\\best_model\\\\checkpoints\\\\checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': \"{'encoder_embed_path': None, 'decoder_embed_path': None}\", 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'raw', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'tmp', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 'hard', 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'C:\\\\Users\\\\hansg\\\\AppData\\\\Local\\\\Temp\\\\tmpzfa6aswm', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': 'raw', 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-04-16 13:23:00 | INFO | fairseq.tasks.translation | [complex] dictionary: 10144 types\n",
      "2023-04-16 13:23:00 | INFO | fairseq.tasks.translation | [simple] dictionary: 10048 types\n",
      "2023-04-16 13:23:00 | INFO | fairseq_cli.generate | loading model(s) from C:\\Users\\hansg\\OneDrive\\NUS\\Classes\\2022-2023-Sem2\\CS5246\\project\\nus_cs5246_project\\facebookresearch\\access\\resources\\models\\best_model\\checkpoints\\checkpoint_best.pt\n",
      "2023-04-16 13:23:06 | INFO | fairseq.data.data_utils | loaded 4 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpzfa6aswm\\tmp.complex-simple.complex\n",
      "2023-04-16 13:23:06 | INFO | fairseq.data.data_utils | loaded 4 examples from: C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpzfa6aswm\\tmp.complex-simple.simple\n",
      "2023-04-16 13:23:06 | INFO | fairseq.tasks.translation | C:\\Users\\hansg\\AppData\\Local\\Temp\\tmpzfa6aswm tmp complex-simple 4 examples\n",
      "2023-04-16 13:23:07 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-04-16 13:23:07 | INFO | fairseq_cli.generate | Translated 4 sentences (69 tokens) in 0.4s (9.87 sentences/s, 170.20 tokens/s)\n"
     ]
    }
   ],
   "source": [
    "access_simp_doc_ST_SUM, access_comp_simp_pairs_ST_SUM = access_simplifier.simplify_documents(st_sum_dict[\"summary_text\"])\n",
    "\n",
    "# Remove ACCESS model from memory\n",
    "del access_simplifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "Document 0:\n",
      "Orig: The Johor Bahru-Singapore Rapid Transit System (RTS) Link is progressing well.\n",
      "-> Simp: The Johor Bahru-Singapore Rapid Transit System (RTS) Link does not work well.\n",
      "Orig: 45 per cent of the work on the Singapore side has been completed.\n",
      "-> Simp: 45% of the work on the Singapore side has been completed.\n",
      "Orig: Transport Minister S. Iswaran provided the update on Friday when he visited the work site.\n",
      "-> Simp: Transport Minister S. Iswaran gave the update on Friday when he went to the work site.\n",
      "Orig: He said: We are on track to achieve the completion goal.\n",
      "-> Simp: He said: We are on track to get the completion goal.\n",
      "\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Document 1:\n",
      "Orig: Bankman-Fried, 31, entered the plea to the new, 13-count indictment through his lawyer.\n",
      "-> Simp: Bankman-Fried, 31, went back to the new, 13-count indictment through his lawyer.\n",
      "Orig: He had earlier pleaded not guilty to eight counts of fraud and conspiracy.\n",
      "-> Simp: He had earlier said that he was not guilty to eight counts of fraud and conspiracy.\n",
      "Orig: He faces a possible sentence of decades in prison if convicted at a trial set to start on Oct 2.\n",
      "-> Simp: He faces a possible sentence of decades in prison for a trial set to start on Oct 2.\n",
      "\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Document 2:\n",
      "Orig: Beijing has been stepping up efforts to woo foreign companies and investors.\n",
      "-> Simp: Beijing has been very important to woo foreign companies and investors.\n",
      "Orig: It has sped up fund licence approvals for foreign asset managers in recent months.\n",
      "-> Simp: It has given money to get money for foreign asset managers in recent months.\n",
      "Orig: Former finance minister: China will encourage foreign capital to participate in its financial markets and may allow foreign-funded financial institutions to go public in the country.\n",
      "-> Simp: Former finance minister: China will encourage foreign capital to take part in its financial markets. They may allow foreign-funded financial institutions to go public in the country.\n",
      "\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Document 3:\n",
      "Orig: Taiwan will have to choose between peace and war, former president Ma Ying-jeou says.\n",
      "-> Simp: The president will have to choose between peace and war, Ma Ying-jeou says.\n",
      "Orig: Mr Ma is the first former Taiwanese president to visit China.\n",
      "-> Simp: Mr Ma is the first former Taiwanese president to go to China.\n",
      "Orig: He was president from 2008 to 2016 as the head of a Kuomintang government.\n",
      "-> Simp: He was president from 2008 to 2016. He was the head of a Kuomintang government.\n",
      "Orig: The party favours close ties with China, which claims the island as its own.\n",
      "-> Simp: The party also had close ties with China, which claims the island as its own.\n",
      "\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Document 4:\n",
      "Orig: Son Heung-min scored twice as South Korea were held to a 2-2 draw by Colombia.\n",
      "-> Simp: Son Heung-min scored twice as South Korea was held to a 2-2 draw by Colombia.\n",
      "Orig: James Rodriguez and Jorge Carrascal scored in the second half to level the scores.\n",
      "-> Simp: James Carrascal and Jorge Carrascal scored in the second half of the score.\n",
      "Orig: Jurgen Klinsmanns first game in charge of South Korea was in Ulsan.\n",
      "-> Simp: Jurgen Klinsmanns first game was in charge of South Korea.\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Save ACCESS outputs\n",
    "save_simplifier_outputs(st_sum_dict[\"summary_text\"], access_simp_doc_ST_SUM, \"access_doc_simp.csv\",\n",
    "                        access_comp_simp_pairs_ST_SUM, \"access_doc_sent_pair_simp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DEPSYM simplifier\n",
    "depsym_simplifier = simplify.create_simplifier(simplify.DEPSYM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 13:23:07 | INFO | stanza | Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b0954e569d47ca900e4a17cb2f28c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 13:23:11 | INFO | stanza | Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| constituency | wsj       |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-04-16 13:23:11 | INFO | stanza | Using device: cuda\n",
      "2023-04-16 13:23:11 | INFO | stanza | Loading: tokenize\n",
      "2023-04-16 13:23:11 | INFO | stanza | Loading: pos\n",
      "2023-04-16 13:23:12 | INFO | stanza | Loading: lemma\n",
      "2023-04-16 13:23:12 | INFO | stanza | Loading: constituency\n",
      "2023-04-16 13:23:13 | INFO | stanza | Loading: depparse\n",
      "2023-04-16 13:23:13 | INFO | stanza | Loading: sentiment\n",
      "2023-04-16 13:23:14 | INFO | stanza | Loading: ner\n",
      "2023-04-16 13:23:15 | INFO | stanza | Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplifying document 1\n",
      "Simplifying document 2\n",
      "Simplifying document 3\n",
      "Simplifying document 4\n",
      "Simplifying document 5\n",
      "Simplifying document 6\n",
      "Simplifying document 7\n",
      "Simplifying document 8\n",
      "Simplifying document 9\n",
      "Simplifying document 10\n",
      "Simplifying document 11\n",
      "Simplifying document 12\n",
      "Simplifying document 13\n",
      "Simplifying document 14\n",
      "Simplifying document 15\n",
      "Simplifying document 16\n",
      "Simplifying document 17\n",
      "Simplifying document 18\n",
      "Simplifying document 19\n",
      "Simplifying document 20\n"
     ]
    }
   ],
   "source": [
    "depsym_simp_doc_ST_SUM, depsym_comp_simp_pairs_ST_SUM = depsym_simplifier.simplify_documents(st_sum_dict[\"summary_text\"])\n",
    "\n",
    "# Remove DEPSYM model from memory\n",
    "del depsym_simplifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "Document 0:\n",
      "Orig: The Johor Bahru-Singapore Rapid Transit System (RTS) Link is progressing well.\n",
      "-> Simp: The Johor Bahru-Singapore rapid transit system (). The Johor Bahru-Singapore rapid transit system are RTS. Link is progressing well.\n",
      "Orig: 45 per cent of the work on the Singapore side has been completed.\n",
      "-> Simp: 45 per cent of the work on the Singapore side has been completed.\n",
      "Orig: Transport Minister S. Iswaran provided the update on Friday when he visited the work site.\n",
      "-> Simp: Transport Minister S. Iswaran provided the update on Friday. This was when he visited the work site.\n",
      "Orig: He said: We are on track to achieve the completion goal.\n",
      "-> Simp: He said: We are on track to achieve the completion goal.\n",
      "\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Document 1:\n",
      "Orig: Bankman-Fried, 31, entered the plea to the new, 13-count indictment through his lawyer.\n",
      "-> Simp: Bankman-fried entered the plea to the new 13-count indictment through his lawyer. Bankman-fried was 31.\n",
      "Orig: He had earlier pleaded not guilty to eight counts of fraud and conspiracy.\n",
      "-> Simp: He had earlier pleaded not guilty to eight counts of fraud and conspiracy.\n",
      "Orig: He faces a possible sentence of decades in prison if convicted at a trial set to start on Oct 2.\n",
      "-> Simp: Then He faces a possible sentence of decades in prison. Suppose He convicted at a trial set to start on Oct 2.\n",
      "\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Document 2:\n",
      "Orig: Beijing has been stepping up efforts to woo foreign companies and investors.\n",
      "-> Simp: Beijing has been stepping up efforts to woo foreign companies and investors.\n",
      "Orig: It has sped up fund licence approvals for foreign asset managers in recent months.\n",
      "-> Simp: It has sped up fund licence approvals for foreign asset managers in recent months.\n",
      "Orig: Former finance minister: China will encourage foreign capital to participate in its financial markets and may allow foreign-funded financial institutions to go public in the country.\n",
      "-> Simp: Former finance minister: China will encourage foreign capital to participate in its financial markets. And Minister will may allow institutions to go public in the country. Institutions are financial. Institutions are foreign-funded. Institutions are financial.\n",
      "\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Document 3:\n",
      "Orig: Taiwan will have to choose between peace and war, former president Ma Ying-jeou says.\n",
      "-> Simp: Taiwan will have to choose between peace and war, former president Ma Ying-jeou says.\n",
      "Orig: Mr Ma is the first former Taiwanese president to visit China.\n",
      "-> Simp: Mr Ma is the first former Taiwanese President. The President to visit China. The President is Taiwanese. The President is former. The President is Taiwanese. The President is first. The President is Taiwanese. The President is former. The President is Taiwanese.\n",
      "Orig: He was president from 2008 to 2016 as the head of a Kuomintang government.\n",
      "-> Simp: He was president from 2008 to 2016 as the head of a Kuomintang government.\n",
      "Orig: The party favours close ties with China, which claims the island as its own.\n",
      "-> Simp: The party favours close ties with China. China which claims the island as its own.\n",
      "\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Document 4:\n",
      "Orig: Son Heung-min scored twice as South Korea were held to a 2-2 draw by Colombia.\n",
      "-> Simp: South Korea were held to a 2-2 draw by Colombia. Son Heung-min scored twice.\n",
      "Orig: James Rodriguez and Jorge Carrascal scored in the second half to level the scores.\n",
      "-> Simp: James Rodriguez and Jorge Carrascal scored in the second half. The second half to level the scores.\n",
      "Orig: Jurgen Klinsmanns first game in charge of South Korea was in Ulsan.\n",
      "-> Simp: Jurgen Klinsmanns game in charge of South Korea was in Ulsan. Game was first.\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Save DEPSYM outputs\n",
    "save_simplifier_outputs(st_sum_dict[\"summary_text\"], depsym_simp_doc_ST_SUM, \"depsym_doc_simp.csv\",\n",
    "                        depsym_comp_simp_pairs_ST_SUM, \"depsym_doc_sent_pair_simp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compare Performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing ACCESS and DEPSYM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplification methods are evaluated on the generated outputs of BART. Only FKGL will be used for quantitatively measuring simplification as a reference simplification is unavailable for the generated summaries. A qualitative analysis of the errors of each method will be in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS FKGL Report:\n",
      "    orig_fkgl  simp_fkgl  reduction ave_reduction\n",
      "0         4.8        3.7   0.229167       0.13108\n",
      "1         6.9        5.7   0.173913              \n",
      "2        12.4        9.5   0.233871              \n",
      "3         6.1        4.0   0.344262              \n",
      "4         6.3        5.9   0.063492              \n",
      "5         6.6        6.8  -0.030303              \n",
      "6         4.6        4.7  -0.021739              \n",
      "7         7.2        7.0   0.027778              \n",
      "8         8.7        7.9   0.091954              \n",
      "9         8.3        7.8   0.060241              \n",
      "10        7.0        6.7   0.042857              \n",
      "11       10.6        6.6   0.377358              \n",
      "12       11.1       10.0   0.099099              \n",
      "13        9.2        9.0   0.021739              \n",
      "14        9.6        8.4   0.125000              \n",
      "15        9.2        8.2   0.108696              \n",
      "16        8.2        7.3   0.109756              \n",
      "17        8.7        6.4   0.264368              \n",
      "18        4.6        4.5   0.021739              \n",
      "19        9.7        7.0   0.278351              \n",
      "\n",
      "DEPSYM FKGL Report:\n",
      "    orig_fkgl  simp_fkgl  reduction ave_reduction\n",
      "0         4.8        5.0  -0.041667      0.175573\n",
      "1         6.9        4.8   0.304348              \n",
      "2        12.4       10.2   0.177419              \n",
      "3         6.1        4.6   0.245902              \n",
      "4         6.3        3.9   0.380952              \n",
      "5         6.6        6.6   0.000000              \n",
      "6         4.6        2.8   0.391304              \n",
      "7         7.2        6.1   0.152778              \n",
      "8         8.7        5.1   0.413793              \n",
      "9         8.3        7.3   0.120482              \n",
      "10        7.0        7.0   0.000000              \n",
      "11       10.6       10.6   0.000000              \n",
      "12       11.1        8.7   0.216216              \n",
      "13        9.2        7.7   0.163043              \n",
      "14        9.6        7.3   0.239583              \n",
      "15        9.2        5.5   0.402174              \n",
      "16        8.2        7.9   0.036585              \n",
      "17        8.7        5.3   0.390805              \n",
      "18        4.6        5.5  -0.195652              \n",
      "19        9.7        8.6   0.113402              \n"
     ]
    }
   ],
   "source": [
    "from textstat import flesch_kincaid_grade\n",
    "import numpy as np\n",
    "\n",
    "def calc_fkgl_report(orig_docs, simp_docs) -> pd.DataFrame:\n",
    "\n",
    "    fkgl_report = dict({\n",
    "        \"orig_fkgl\": [],\n",
    "        \"simp_fkgl\": [],\n",
    "        \"reduction\": [],\n",
    "        \"ave_reduction\": []\n",
    "    })\n",
    "\n",
    "    assert len(orig_docs) == len(simp_docs)\n",
    "\n",
    "    # Calculate the FKGL scores of the original document and the simplified document\n",
    "    doc_cnt = len(orig_docs)\n",
    "    for doc_idx in range(doc_cnt):\n",
    "        orig_doc = orig_docs[doc_idx]\n",
    "        simp_doc = simp_docs[doc_idx]\n",
    "        fkgl_report[\"orig_fkgl\"].append(flesch_kincaid_grade(orig_doc))\n",
    "        fkgl_report[\"simp_fkgl\"].append(flesch_kincaid_grade(simp_doc))\n",
    "\n",
    "    # Calculate how much smaller the fkgl of the simplified document is\n",
    "    reduction = [ 1 - (simp_fkgl/orig_fkgl)  for orig_fkgl, simp_fkgl in zip(fkgl_report[\"orig_fkgl\"], fkgl_report[\"simp_fkgl\"]) ]\n",
    "    fkgl_report[\"reduction\"] = reduction \n",
    "\n",
    "    # Calculate the average reduction. Just put in the first cell.\n",
    "    ave_reduction = np.mean(reduction)\n",
    "    for _ in range(len(reduction)):\n",
    "        fkgl_report[\"ave_reduction\"].append(\"\") # Empty values for padding\n",
    "    fkgl_report[\"ave_reduction\"][0] = ave_reduction \n",
    "\n",
    "    # Turn to a pandas dataframe\n",
    "    fkgl_report = pd.DataFrame.from_dict(fkgl_report)\n",
    "\n",
    "    return fkgl_report\n",
    "\n",
    "access_fkgl_ST_SUM = calc_fkgl_report(st_sum_dict['summary_text'], access_simp_doc_ST_SUM)\n",
    "depsym_fkgl_ST_SUM = calc_fkgl_report(st_sum_dict['summary_text'], depsym_simp_doc_ST_SUM)\n",
    "print(\"ACCESS FKGL Report:\")\n",
    "print(access_fkgl_ST_SUM)\n",
    "access_fkgl_ST_SUM.to_csv(\"access_fkgl_report.csv\", index=False)\n",
    "print()\n",
    "print(\"DEPSYM FKGL Report:\")\n",
    "print(depsym_fkgl_ST_SUM)\n",
    "depsym_fkgl_ST_SUM.to_csv(\"depsym_fkgl_report.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
